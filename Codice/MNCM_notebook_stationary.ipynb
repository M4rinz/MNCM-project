{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consinstently estimating Markov Chains with Noisy Aggregated Data\n",
    "\n",
    "Notebook con esperimenti numerici per il seminario di fine corso di Metodi Numerici per le Catene di Markov (versione stazionaria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid the pain of restarting the kernel each time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic stuff\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utilities.estimators import P_mom_stationary, P_cls_stationary\n",
    "from utilities.data import generate_random_P, create_observations\n",
    "from utilities.num_methods import compute_stationary_LU_GTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fissiamo il numero di stati\n",
    "S = 10\n",
    "\n",
    "# Fissiamo il seed per la riproducibilità degli esperimenti\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the path where to load/store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBDIRECTORY = f'experiment_seed={SEED}'\n",
    "PATH = os.path.join('data',SUBDIRECTORY)\n",
    "\n",
    "# Let's create the directory, if it doesn't exist yet\n",
    "os.makedirs(PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment n° 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first experiment, we want to examine the behaviour of the MoM and CLS estimators for different values of $T$ and $K$. \n",
    "\n",
    "In particular, we want to plot the approximation error against $T\\times K$. The approximation error is measured as $\\frac{1}{S^2}{\\lvert\\lvert\\hat{P}-P\\rvert\\rvert}_F^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate $P$ and $\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do is to generate the true transition matrix $P\\in\\mathbb{R}^{S\\times S}$.\n",
    "\n",
    "Just like the authors, we choose $S=10$, and each row will have a Dirichlet distribution: $P[i,\\colon]\\sim\\operatorname{Dirichlet}\\left(\\frac{D}{S}\\mathbf{e}\\right)$, where $D$ is the precision parameter and $\\mathbf{e}$ is the vector of all ones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D=0.5 (like in the article)\n",
    "D = 0.5\n",
    "P = generate_random_P(S,'dirichlet',precision=D,rng=np.random.default_rng(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "fig.suptitle('P matrix', fontsize=16)\n",
    "plt.title(f\"Mean: 1/{S}*e, precision: {D}\")\n",
    "\n",
    "sns.heatmap(P, ax=ax, cmap='viridis', annot=True, fmt=\".1e\", annot_kws={\"size\": 8}, cbar=False, xticklabels=False, yticklabels=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to compute the steady-state vector $\\pi$, because in the stationary case we use that as initial distribution.\n",
    "\n",
    "As seen during the course, there are many ways to do it. Since the size of $P$ is modest, I go for a direct method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial distribution computation\n",
    "\n",
    "pi_gth= compute_stationary_LU_GTH(P)\n",
    "print(f\"The norm of pi_gth @ (I-P) = {np.linalg.norm(pi_gth@(np.eye(S)-P))}\")\n",
    "\n",
    "# We set the initial distribution\n",
    "pi_0 = pi_gth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the components of the invariant distribution $\\pi$. Its $i$-th component is related to the probability of having observations in state $i$ for the initial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(6,5))\n",
    "fig.suptitle(r\"Components of $\\pi$\", fontsize=16, fontweight='bold')\n",
    "\n",
    "ax.plot(pi_0, linestyle=':', marker='o')\n",
    "ax.set_xlabel('Component', fontsize=12)\n",
    "ax.set_ylabel('value (log)', fontsize=12)\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the transition matrix and its invariant distribution, just for consistency, i.e. to make sure that we're actually using the $P$ and $\\pi$ that we just produced in the next computations. This may be needed in case of partial execution of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH,f'P__SEED={SEED}.npy'),'wb') as file:\n",
    "\tnp.save(file, P)\n",
    "\n",
    "with open(os.path.join(PATH,f'pi__SEED={SEED}.npy'),'wb') as file:\n",
    "\tnp.save(file, pi_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the data (i.e. simulate the Markov Chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $N\\in\\mathbb{N}, p\\in\\mathbb{R}^S$. The multinomial distribution $\\mathrm{Multinomial}(N,p)$ can be intuitively explained as the outcome of $N$ independent repeated trials, each having $S$ possible outcomes, such that the outcome $i$ has probability $p_i$ of occurring. \n",
    "\n",
    "For example, the throw of 2 fair 20-sided dice can be modeled with a random variable $X\\sim\\mathrm{Multinomial}(6,\\frac{1}{20}\\mathbf{e})$.\n",
    "\n",
    "In such a context, a binomial distribution with parameters $N$ (n° of trials) and $\\alpha$ (probability of success) would just be $\\mathrm{Multinomial}(N,(\\alpha, 1-\\alpha))$\n",
    "\n",
    "**OSS**: Let $\\bar{X}$ is a sample from $\\mathrm{Multinomial}(N,p)$, then $\\bar{X}\\in\\mathbb{R}^S$, and it's easy to verify that $\\bar{X}_i$ is binomial of parameters $N$ and $p_i$, in fact $\\bar{X}_i\\sim\\mathrm{Multinomial}(N,p_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `create_observations` generates the data, and automatically adds noise to it, if specified.\n",
    "\n",
    "The initial distribution of the population is generated by drawing $n_1\\sim\\mathrm{Multinomial}(N,\\pi)$. $K$ independent observations are generated at the beginning.\n",
    "\n",
    "**OSS**: We're in the (strongly) stationary setting, the initial distribution is the steady-state vector $\\pi$. The idea is that $\\pi_i$ is the probability of having a single observation (i.e. a single individual presence) at state $i$.\n",
    "\n",
    "For the following timesteps, we have\n",
    "$$\n",
    "\tn_{t+1}\\sim\\sum_{i=0}^S\\mathrm{Multinomial}\\left(n_t(i),P[i\\mid\\colon]\\right)\\quad\\forall\\, t\\in[T-1]\n",
    "$$\n",
    "The idea is that at timestep $t$ there are $n_t(i)$ members of the population, each will move to another state according to $P[i\\mid\\colon]$ in the next timestep. \n",
    "\n",
    "Again, $K$ independent observations can be generated by the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OSS**: Let `n_array, y_array, A = create_observations(...)` \n",
    "\n",
    "Then `n_array` and `y_array` are `np.ndarray`s of shape $T\\times K\\times S$. \n",
    "\n",
    "They can be thought as lists of length $T$ in which the item at position $t\\in\\{0,\\dots,T-1\\}$ is a $K\\times S$ `np.ndarray` that contains the $K$ observations for timestep $t+1$.\n",
    "\n",
    "Instead, `A` is a $S$ by $S$ `np.ndarray`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining the **parameters range**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of the error's distribution parameters\n",
    "alpha = [1, 0.5, 0.25]\n",
    "variance = [0, 1, 5]\n",
    "\n",
    "# ranges\n",
    "T_range = [10**k for k in range(1,5)]\t#range(1,3)\n",
    "K_range = [1, 2, 5, 10, 20, 50]\t\t\t#[1, 5, 20, 50]\n",
    "\n",
    "# other parameters\n",
    "n_reps = 10\n",
    "N = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by **loading** the transition matrix and its invariant distribution. Just for safety (and good practice I guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH,f'P__SEED={SEED}.npy'),'rb') as file:\n",
    "\tP = np.load(file)\n",
    "\n",
    "with open(os.path.join(PATH,f'pi__SEED={SEED}.npy'),'rb') as file:\n",
    "\tpi_0 = np.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the **data creation** is an expensive process, let's do it in a separate cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.data import return_subdir_name, save_observation\n",
    "\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "prod = product(T_range, K_range, range(len(alpha)), ['gaussian','binomial'])\n",
    "for T, K, i, noise_type in tqdm(prod):\n",
    "\t# Get parameter, distinguishing the two cases\n",
    "\tparameter = np.sqrt(variance[i]) if noise_type=='gaussian' else alpha[i]\n",
    "\t\n",
    "\t# Create subdirectory name according to a standard\n",
    "\tsubdir_name = return_subdir_name(T=T, K=K, S=S, N=N,\n",
    "\t\t\t\t\t\t\t\t  noise_type=noise_type,\n",
    "\t\t\t\t\t\t\t\t  parameter=parameter)\n",
    "\t# Create the path of the subdirectory in which the arrays corresponding to\n",
    "\t# the given configurations of parameters have to be stored\n",
    "\tsubdir_path = os.path.join(PATH,'observations',subdir_name)\n",
    "\n",
    "\t# Invariant: the noisy and original observations are either both present\n",
    "\t# or not present.\n",
    "\t# We check that the subdirectory doesn't exist, or that it exists but is empty\n",
    "\tif not os.path.exists(subdir_path) or not os.listdir(subdir_path):\n",
    "\t\tos.makedirs(subdir_path, exist_ok=True)\n",
    "\n",
    "\t\t# Create the n_reps observations to fill the folder right away\n",
    "\t\tfor rep in range(n_reps):\n",
    "\t\t\t# Create name of the file in which to save the arrays\n",
    "\t\t\tn_filename = f\"n_t_arr__repetition={rep}\"\n",
    "\t\t\ty_filename = f\"y_t_arr__repetition={rep}\"\n",
    "\t\t\t# This is \"complicazione affari semplici\"\n",
    "\t\t\t# At the very least let's save one copy instead of n_reps\n",
    "\t\t\tpar_name_val = \"stdev\" if noise_type=='gaussian' else 'alpha'\n",
    "\t\t\tA_filename = f\"A_noise_type={noise_type}_{par_name_val}={parameter}\"\n",
    "\t\t\n",
    "\t\t\tn_t_array, y_t_array, A = create_observations(\n",
    "\t\t\t\t\t\t\t\t\t\t\tT=T, K=K, N=N,\n",
    "\t\t\t\t\t\t\t\t\t\t\tP=P, pi_0=pi_0,\n",
    "\t\t\t\t\t\t\t\t\t\t\tnoise_type=noise_type, \n",
    "\t\t\t\t\t\t\t\t\t\t\tparameter=parameter)\n",
    "\n",
    "\t\t\tsave_observation(array=n_t_array,\n",
    "\t\t\t\t\t\t\tfilename = n_filename,\n",
    "\t\t\t\t\t\t\tpath=subdir_path)\n",
    "\t\t\t\n",
    "\t\t\tsave_observation(array=y_t_array,\n",
    "\t\t\t\t\t\t\tfilename = y_filename,\n",
    "\t\t\t\t\t\t\tpath=subdir_path)\n",
    "\t\t\t\n",
    "\t\t\tsave_observation(array=A,\n",
    "\t\t\t\t\t\t\tfilename=A_filename,\n",
    "\t\t\t\t\t\t\tpath=subdir_path)\n",
    "\telse:\n",
    "\t\t# In case the directory already exists and has files inside, \n",
    "\t\t# we just check that there's the correct number of them\n",
    "\t\tnum_files = len(os.listdir(subdir_path))\n",
    "\t\tassert num_files == 2*n_reps+1, f\"Error: in {subdir_name} there are {num_files} files, there should be 2*n_+1 reps = {2*n_reps}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the estimators and their error, using the data we stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.data import load_observation\n",
    "\n",
    "gauss_errors, binomial_errors = [], []\n",
    "zero_division_configurations = []\n",
    "\n",
    "def error_computation(M:np.ndarray) -> float: \n",
    "    sqnorm = (np.linalg.norm(M-P,'fro')**2)\n",
    "    return sqnorm/(S**2)\n",
    "\n",
    "prod = product(T_range, K_range, range(len(alpha)), ['gaussian','binomial'])\n",
    "for T, K, i, noise_type in tqdm(prod):\n",
    "    # Get parameter, distinguishing the two cases\n",
    "\tparameter = np.sqrt(variance[i]) if noise_type=='gaussian' else alpha[i]\n",
    "\t# Create a dictionary to store the data, with parameters value\n",
    "\tdict_entry = {\n",
    "\t\t'noise_type': noise_type,\n",
    "\t\t'TxK': T*K,\n",
    "\t}\n",
    "\tif noise_type == 'gaussian':\n",
    "\t\tdict_entry['stdev'] = parameter\n",
    "\telse:\n",
    "\t\tdict_entry['alpha'] = parameter\n",
    "\n",
    "\t# Get the name of the subdirectory relative to the current params config\n",
    "\tobs_subdir_name = return_subdir_name(T=T, K=K, S=S, N=N,\n",
    "\t\t\t\t\t\t\t\t  noise_type=noise_type,\n",
    "\t\t\t\t\t\t\t\t  parameter=parameter)\n",
    "\t# Get the path of the subdirectory\n",
    "\tobs_subdir_path = os.path.join(PATH,'observations',obs_subdir_name)\n",
    "\n",
    "\t# We check that the path exists and has data.\n",
    "\t# This could be done more elegantly with exceptions maybe, but should be ok\n",
    "\tif not os.path.exists(obs_subdir_path) or not os.listdir(obs_subdir_path):\n",
    "\t\tprint(f\"The directory {obs_subdir_path} is empty or doesn't exist\")\n",
    "\t\tprint(\"Please generate the data by running the cell above\")\n",
    "\t\tbreak\n",
    "\n",
    "\t# Just like the author, the computations are repeated \n",
    "\t# (of course we already have the data)\n",
    "\tfor rep in range(n_reps):\n",
    "\t\t# Get name of the file in which the arrays are saved\n",
    "\t\tn_filename = f\"n_t_arr__repetition={rep}\"\n",
    "\t\ty_filename = f\"y_t_arr__repetition={rep}\"\n",
    "\t\tpar_name_val = \"stdev\" if noise_type=='gaussian' else 'alpha'\n",
    "\t\tA_filename = f\"A_noise_type={noise_type}_{par_name_val}={parameter}\"\n",
    "\n",
    "\t\tn_t_array = load_observation(filename=n_filename, path=obs_subdir_path)\n",
    "\t\ty_t_array = load_observation(filename=y_filename, path=obs_subdir_path)\n",
    "\t\tA_t_arr = load_observation(filename=A_filename, path=obs_subdir_path)\n",
    "\n",
    "\t\twith warnings.catch_warnings(record=True) as w:\n",
    "\t\t\t# I'm being lazy and catching all warnings, but all I really care is \n",
    "\t\t\t# RuntimeWarning: invalid value encountered in divide\n",
    "\t\t\twarnings.simplefilter(\"always\")\n",
    "\n",
    "\t\t\t# method of moments estimator\n",
    "\t\t\tP_mom, mu_hat, _ = P_mom_stationary(y_array=y_t_array, A=A_t_arr, N=N)\n",
    "\t\t\t# conditional least squares estimator\n",
    "\t\t\tP_cls = P_cls_stationary(y_array=y_t_array)\n",
    "\n",
    "\t\t\t# In case or zero divisions during P_mom computation, err=np.nan\n",
    "\t\t\t# Thus, something may be made about it\n",
    "\t\t\terr_mom = error_computation(P_mom)\n",
    "\t\t\terr_cls = error_computation(P_cls)\n",
    "\t\t\t\n",
    "\t\t\tdict_entry[f'error_MoM_{rep}'] = err_mom\n",
    "\t\t\tdict_entry[f'error_CLS_{rep}'] = err_cls\n",
    "\n",
    "\t\t\t# Store the error, with its parameters, into the list\n",
    "\t\t\tif noise_type=='gaussian':\n",
    "\t\t\t\tgauss_errors.append(dict_entry)\n",
    "\t\t\telse:\n",
    "\t\t\t\tbinomial_errors.append(dict_entry)\n",
    "\n",
    "\t\t\tif w:\n",
    "\t\t\t\tbad_config = {\n",
    "\t\t\t\t\t'T':T,\n",
    "\t\t\t\t\t'K':K,\n",
    "\t\t\t\t\t'noise_type':noise_type,\n",
    "\t\t\t\t\t'n_t': n_t_array,\n",
    "\t\t\t\t\t'y_t': y_t_array,\n",
    "\t\t\t\t\t'A_t': A_t_arr,\n",
    "\t\t\t\t\t'parameter': parameter,\n",
    "\t\t\t\t\t'P_mom': P_mom,\n",
    "\t\t\t\t\t'mu_hat': mu_hat\n",
    "\t\t\t\t}\n",
    "\t\t\t\tzero_division_configurations.append(bad_config)\n",
    "\t\t\t\n",
    "\t\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `NaN`s checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with the `NaN`s, and I don't think we'll ever get rid of them.\n",
    "\n",
    "The problem is that sometimes there are no individuals in a certain state, at all. Thus the sample mean $m$ has a zero component, and in the formula for the estimator\n",
    "$$\n",
    "\t\\hat{P}_{\\textup{MoM}}:=\\operatorname{Diag}(\\hat{\\mu})^{-1}\\left(\\frac{1}{N}\\hat{\\Sigma}+\\hat{\\mu}\\cdot\\hat{\\mu}^{\\top}\\right)\n",
    "$$\n",
    "there is a division by zero.\n",
    "\n",
    "Of course, this is unlikely. The factor that make it more likely are: \n",
    "- No noise\n",
    "- Few total timesteps $T$, few repeated observations $K$\n",
    "- $\\pi_i$ is very small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check these claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_outcomes_df = pd.DataFrame(zero_division_configurations)\n",
    "\n",
    "bad_gaussian_df = bad_outcomes_df[bad_outcomes_df['noise_type'] == 'gaussian']\n",
    "bad_binomial_df = bad_outcomes_df[bad_outcomes_df['noise_type'] == 'binomial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which noise is more problematic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, let's see which noise model yielded the most divisions by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows for each noise type\n",
    "noise_counts = bad_outcomes_df['noise_type'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "sns.barplot(x=noise_counts.index,  y=noise_counts.values, \n",
    "\t\t\tpalette=['orange', 'teal'], edgecolor='black',\n",
    "\t\t\tlegend=False, hue=noise_counts.index,\n",
    "\t\t\tax=ax)\n",
    "\n",
    "# Display the absolute number in the middle of each bar\n",
    "for i in range(len(noise_counts)):\n",
    "\tax.text(i, noise_counts.values[i] / 2, str(noise_counts.values[i]), ha='center', va='center', fontsize=12, color='white')\n",
    "\n",
    "ax.set_xlabel('Noise Type', fontsize=12)\n",
    "ax.set_ylabel('Count', fontsize=12)\n",
    "ax.set_title('Number of bad outcomes for each noise type', fontsize=14, fontweight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimators with `NaN` components are disproportionately due to binomial rather than gaussian noise. We'll return on this point when discussing the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Zero divisions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing first, let's give evidence that indeed this is due to a zero division (I know it because I read the alerts raised by NumPy, but their hidden in this version of the code).\n",
    "\n",
    "Is it true that all the computed means $\\hat{m}$ have a zero component somewhere?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mu_have_a_zero = bad_outcomes_df['mu_hat'].apply(lambda arr: np.any(arr == 0)).all()\n",
    "\n",
    "if all_mu_have_a_zero:\n",
    "\tprint(\"All the mean vectors (computed from the noisy observations, mind you) have a zero somewhere\")\n",
    "else:\n",
    "\tprint(\"At least one mean vector has all nonzero components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ranges for $T$ and $K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remind that the means are taken across the $T$ (temporal) and $K$ (repetitions) dimension. So, if there is a zero in component $i$ of the mean vector, this means (ignoring noise for now, but we'll come back to it later) that all of the addends are zero.\n",
    "\n",
    "Of course, this is really unlikely, unless the addends are few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle(\"Distribution of T and K values\", fontweight='bold', fontsize=22)\n",
    "\n",
    "# Plot for bad_gaussian_df T values\n",
    "sns.barplot(x=bad_gaussian_df['T'].value_counts().index, \n",
    "\t\t\ty=bad_gaussian_df['T'].value_counts().values, \n",
    "\t\t\tax=ax[0, 0], \n",
    "\t\t\tcolor='orange', edgecolor='black')\n",
    "#sns.histplot(bad_gaussian_df['T'], ax=ax[0,0], color='orange', discrete=True)\n",
    "ax[0, 0].set_title('Distribution of T values (Gaussian)', fontsize=18)\n",
    "ax[0, 0].set_xlabel('T', fontsize=14)\n",
    "ax[0, 0].set_ylabel('Frequency', fontsize=14)\n",
    "#xticks = sorted(set(ax[0, 0].get_xticks()[:-1]).union(set(bad_gaussian_df['T'].unique())))\n",
    "#ax[0, 0].set_xticks(xticks)\n",
    "\n",
    "# Plot for bad_gaussian_df K values\n",
    "sns.histplot(bad_gaussian_df['K'], ax=ax[0, 1], color='orange', discrete=True)\n",
    "ax[0, 1].set_title('Distribution of K values (Gaussian)', fontsize=18)\n",
    "ax[0, 1].set_xlabel('K', fontsize=14)\n",
    "ax[0, 1].set_ylabel('Frequency', fontsize=14)\n",
    "xticks = sorted(set(ax[0, 1].get_xticks()[1:]).union(set(bad_gaussian_df['K'].unique())))\n",
    "ax[0, 1].set_xticks([int(x) for x in xticks])\n",
    "\n",
    "# Plot for bad_binomial_df T values\n",
    "sns.barplot(x=bad_binomial_df['T'].value_counts().index, \n",
    "\t\t\ty=bad_binomial_df['T'].value_counts().values, \n",
    "\t\t\tax=ax[1, 0], \n",
    "\t\t\tcolor='teal', edgecolor='black')\n",
    "ax[1, 0].set_title('Distribution of T values (Binomial)', fontsize=18)\n",
    "ax[1, 0].set_xlabel('T', fontsize=14)\n",
    "ax[1, 0].set_ylabel('Frequency', fontsize=14)\n",
    "#xticks = sorted(set(ax[1, 0].get_xticks()[:-1]).union(set(bad_binomial_df['T'].unique())))\n",
    "#ax[1, 0].set_xticks(xticks)\n",
    "\n",
    "# Plot for bad_binomial_df K values\n",
    "sns.histplot(bad_binomial_df['K'], ax=ax[1, 1], color='teal', discrete=True)\n",
    "ax[1, 1].set_title('Distribution of K values (Binomial)', fontsize=18)\n",
    "ax[1, 1].set_xlabel('K', fontsize=14)\n",
    "ax[1, 1].set_ylabel('Frequency', fontsize=14)\n",
    "xticks = sorted(set(ax[1, 1].get_xticks()[1:]).union(set(bad_binomial_df['K'].unique())))\n",
    "ax[1, 1].set_xticks(xticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that for both noise models the observations that yield division by zero have low values of $T$. \n",
    "\n",
    "This is consistent with the hypothesis that by evolving for more timesteps, sooner or later someone will land in every state. Conversely, if the timesteps are few it is increasingly likely that there are states without individuals throughout the whole simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the number of repetitions $K$ is low, and a similar reasoning as before can be made. When the repetitions are many, it's less likely to encounter states without individuals in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Noise parameters check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by seeing what were the values of the noise parameters corresponding to zero divisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Unique values for the standard deviation that lead to zero division in the gaussian noise case: {bad_gaussian_df['parameter'].unique()}\")\n",
    "print(f\"Unique values for the standard deviation that lead to zero division in the binomial noise case: {bad_binomial_df['parameter'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle(\"Noise parameter values of the observations with bad outcome\")\n",
    "\n",
    "#sns.barplot(x=bad_gaussian_df['parameter'].value_counts().index, \n",
    "#\t\t\ty=bad_gaussian_df['parameter'].value_counts().values, \n",
    "#\t\t\tcolor='orange', edgecolor='black', ax=ax[0])\n",
    "sns.histplot(bad_gaussian_df['parameter'], color='orange', bins=10, ax=ax[0])\n",
    "ax[0].set_xlim(left=0)\n",
    "ax[0].set_title('Distribution of parameter values (Gaussian)')\n",
    "ax[0].set_xlabel('Parameter')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "xticks = sorted(set(ax[0].get_xticks()).union(set(bad_gaussian_df['parameter'].unique())))\n",
    "ax[0].set_xticks(xticks)\n",
    "\n",
    "sns.barplot(x=bad_binomial_df['parameter'].value_counts().index, \n",
    "\t\t\ty=bad_binomial_df['parameter'].value_counts().values, \n",
    "\t\t\tcolor='teal', edgecolor='black', ax=ax[1])\n",
    "ax[1].set_title('Distribution of parameter values (Binomial)')\n",
    "ax[1].set_xlabel('Parameter')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "#xticks = sorted(set(ax[1].get_xticks()).union(set(bad_binomial_df['parameter'].unique())))\n",
    "#ax[1].set_xticks(xticks)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the gaussian noise, we only get divisions by zero when the variance is zero, i.e. no noise is added and $y_t=n_t$. It makes sense that $y_t(i)\\ne 0$ in the general case, because the probability of summing exactly zero (i.e. drawing $\\epsilon_t=0$) to $n_t(i)$ is zero.\n",
    "\n",
    "**OSS**: when the observations are not noisy, $\\hat{P}_{\\textup{MoM}}$ is stochastic, but there is the risk of not obtaining anything useful (i.e. encounter divisions by zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn to the binomial noise. \n",
    "\n",
    "In the case $\\alpha=1$ then there's no noise and $n_t=y_t$, so for this case the divisions by zero are \"inherited\" from the original aggregated data (no individuals in one of the state throughout the all evolution, for all repetitions).\n",
    "\n",
    "However, when the detection probability $\\alpha$ is small, there is the chance that $n_t(i)$ is small and $y_t(i)=0$, as $\\mathbb{P}\\left(y_t(i)=0\\mid n_t(i)\\right)=(1-\\alpha)^{n_t(i)}$.\n",
    "\n",
    "This is why most observations that give divisions by zero have gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's move on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some dataframes and compute the means beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes\n",
    "gauss_df = pd.DataFrame(gauss_errors)\n",
    "binom_df = pd.DataFrame(binomial_errors)\n",
    "\n",
    "# Compute the means of the repeated observations\n",
    "gauss_df['mean_MoM'] = gauss_df[[f'error_MoM_{i}' for i in range(n_reps)]].mean(axis=1).dropna()\n",
    "gauss_df['mean_CLS'] = gauss_df[[f'error_CLS_{i}' for i in range(n_reps)]].mean(axis=1).dropna()\n",
    "binom_df['mean_MoM'] = binom_df[[f'error_MoM_{i}' for i in range(n_reps)]].mean(axis=1).dropna()\n",
    "binom_df['mean_CLS'] = binom_df[[f'error_CLS_{i}' for i in range(n_reps)]].mean(axis=1).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's peek at what we've got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binom_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "for i, noise_type in enumerate(['gaussian', 'binomial']):\n",
    "    data_che_uso = gauss_df if noise_type == 'gaussian' else binom_df\n",
    "    small_df = data_che_uso.copy()\n",
    "    small_df.drop(columns=[f'error_CLS_{i}' for i in range(n_reps)],inplace=True)\n",
    "    hue_val = 'stdev' if noise_type == 'gaussian' else 'alpha'\n",
    "    # Plot the errors\n",
    "    sns.lineplot(data=small_df, x='TxK', y='mean_MoM', \n",
    "                 err_style='bars', errorbar='ci',\n",
    "                 hue=hue_val, ax=ax[i], marker='o')\n",
    "    # Plot 1/(T*K)\n",
    "    ax[i].plot(data_che_uso['TxK'], 1/data_che_uso['TxK'], \n",
    "               linestyle='--', color='green', alpha = 0.5,\n",
    "               label='1/(T*K)')\n",
    "    \n",
    "    ax[i].set_xscale('log')\n",
    "    ax[i].set_yscale('log')\n",
    "    ax[i].set_xlabel('TxK', fontsize=12)\n",
    "    ax[i].set_ylabel('Mean Square Error', fontsize=12)\n",
    "    ax[i].set_title(f'P estimation error vs TxK ({noise_type} noise)', fontsize=16)\n",
    "    ax[i].legend()\n",
    "\n",
    "fig.suptitle('P estimation error, Method of Moments', fontsize=18)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(15,5))\n",
    "fig.suptitle('P estimation error, CLS estimator', fontsize=18)\n",
    "\n",
    "for i, noise_type in enumerate(['gaussian', 'binomial']):\n",
    "    data_che_uso = gauss_df if noise_type == 'gaussian' else binom_df\n",
    "    small_df = data_che_uso.copy()\n",
    "    small_df.drop(columns=[f'error_MoM_{i}' for i in range(n_reps)],inplace=True)\n",
    "    hue_val = 'stdev' if noise_type == 'gaussian' else 'alpha'\n",
    "    # Plot the errors\n",
    "    sns.lineplot(data=small_df, x='TxK', y='mean_CLS', \n",
    "                 err_style='bars', errorbar='ci',\n",
    "                 hue=hue_val, ax=ax[i], marker='o')\n",
    "    # Plot 1/(T*K)\n",
    "    ax[i].plot(data_che_uso['TxK'], 1/data_che_uso['TxK'], \n",
    "               linestyle='--', color='green', alpha = 0.5,\n",
    "               label='1/(T*K)')\n",
    "    \n",
    "    ax[i].set_xscale('log')\n",
    "    ax[i].set_yscale('log')\n",
    "    ax[i].set_xlabel('TxK', fontsize=12)\n",
    "    ax[i].set_ylabel('Mean Square Error', fontsize=12)\n",
    "    ax[i].set_title(f'P estimation error vs TxK ({noise_type} noise)', fontsize=16)\n",
    "    ax[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment n° 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third experiment, we assess the impact of the population size $N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_range = [10**k for k in range(0,5)]\n",
    "T = 10**5\n",
    "K = 20\n",
    "\n",
    "n_reps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_errors = []\n",
    "\n",
    "for N in tqdm(N_range):\n",
    "    dict_entry = {'N':N}\n",
    "\n",
    "    for rep in range(n_reps):\n",
    "        n_array, _, _ = create_observations(T=T,N=N,K=K,pi_0=pi_0,stationary=True)\n",
    "\n",
    "        P_mom = P_mom_stationary(y_array = n_array, A = np.eye(S), N=N)\n",
    "        P_cls = P_cls_stationary(y_array = n_array)\n",
    "\n",
    "        dict_entry[f'error_MoM_{rep}'] = error_computation(P_mom)\n",
    "        dict_entry[f'error_CLS_{rep}'] = error_computation(P_cls)\n",
    "\n",
    "    N_errors.append(dict_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_df = pd.DataFrame(N_errors)\n",
    "N_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(20,5))\n",
    "\n",
    "for i, estimator in enumerate(['MoM', 'CLS']):\n",
    "    y = N_df[[f'error_{estimator}_{i}' for i in range(n_reps)]].mean(axis=1).dropna()\n",
    "    sns.lineplot(data=N_df, x='N', y=y, ax=ax[i])\n",
    "    ax[i].set_xscale('log')\n",
    "    ax[i].set_yscale('log')\n",
    "    ax[i].set_xlabel('N', fontsize=14)\n",
    "    ax[i].set_ylabel('Mean Square Error', fontsize=14)\n",
    "    ax[i].set_title(f'P estimation MSE vs N ({estimator} estimator)', fontsize=16)\n",
    "\n",
    "fig.suptitle('P estimation error', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the error has the same order of magnitude, independently from $N$. This is in accordance with what's been found by the authors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
