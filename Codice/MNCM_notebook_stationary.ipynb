{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consinstently estimating Markov Chains with Noisy Aggregated Data\n",
    "\n",
    "## Notebook con esperimenti numerici per il seminario di fine corso di Metodi Numerici per le Catene di Markov (versione stazionaria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic stuff\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities import P_mom_stationary, add_noise, generate_random_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of repeated observations\n",
    "K = 30\n",
    "# Population size\n",
    "N = 100\n",
    "# Number of states\n",
    "S = 20\n",
    "# Number of timesteps\n",
    "T = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the non-stationary case, we now have to compute the steady state vector $\\pi$ of the Markov chain. This is the thing in the seminar that is the most related to the course.\n",
    "\n",
    "Since the size of the matrix $P$ is modest, I go for a direct method: LU factorization with the GTH trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stationary_LU_rough(P:np.ndarray) -> tuple[np.ndarray, float, float]:\n",
    "    #let's start very roughly: solve (I-P.T)@pi = 0 via LU factorization\n",
    "    S = P.shape[0]\n",
    "    A = np.eye(S)-P.T\n",
    "    M,L,U = scipy.linalg.lu(A)\n",
    "    LU_error = np.linalg.norm(M@L@U - A)\n",
    "    pi = np.zeros(S)\n",
    "    pi[-1] = 1\n",
    "    for i in range(S-2,-1,-1):\n",
    "        pi[i] = (-U[i,i+1:]@pi[i+1:])/U[i,i]\n",
    "    pi /= pi.sum()\n",
    "    res = np.linalg.norm(A@pi)\n",
    "    return pi, res, LU_error\n",
    "\n",
    "#TODO\n",
    "#def compute_stationary_LU_GTH(P:np.ndarray) -> np.ndarray:\n",
    "#    S = P.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The norm of (I-P.T)@pi is 1.3218586933369176e-16\n"
     ]
    }
   ],
   "source": [
    "# True transition matrix\n",
    "P = generate_random_P(S,'dirichlet',precision=0.5)\n",
    "\n",
    "# Initial distribution\n",
    "pi_rough, res, _ = compute_stationary_LU_rough(P)\n",
    "print(f'The norm of (I-P.T)@pi is {res}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the observed data. We immediately generate the $K$ observations.\n",
    "\n",
    "This is done by generating $n_t\\sim\\mathrm{Multinomial}(N,\\pi)$ for each $t\\in[T]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_0 = pi_rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_t = pi_0.T\n",
    "n_t_vector = []\n",
    "y_t_vector = []\n",
    "for t in range(T):\n",
    "    # create K observations of the observed data \n",
    "    # (multinomial draw from the marginal distribution)\n",
    "    n_t = np.random.multinomial(n=N, pvals=mu_t, size=K)\n",
    "    # create noisy observations\n",
    "    y_t, _ = add_noise(n_t)\n",
    "    # append the observations\n",
    "    n_t_vector.append(n_t)\n",
    "    y_t_vector.append(y_t)\n",
    "    # In the stationary case, the marginal distribution is equal \n",
    "    # to the stationary distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OSS**: `n_t_vector` and `y_t_vector` are lists of length $T$, the item in the list in position $t\\in\\{0,\\dots,T-1\\}$ is a $K\\times S$ `np.ndarray` that contains the $K$ observations for timestep $t+1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimators of $P$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_array = np.array(y_t_vector)\n",
    "A = np.eye(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_mom.shape = (20, 20)\n",
      "The 2-norm of the difference between P and P_mom is 3.5870520250972224\n"
     ]
    }
   ],
   "source": [
    "P_mom = P_mom_stationary(y_array=y_array, A=A, N=N)\n",
    "print(f'P_mom.shape = {P_mom.shape}')\n",
    "print(f'The 2-norm of the difference between P and P_mom is {np.linalg.norm(P-P_mom)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the Conditional Least Squares estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape = (30, 1999, 20),\t (K, T-1, S) = (30, 1999, 20)\n"
     ]
    }
   ],
   "source": [
    "X = y_array[:-1].transpose(1,0,2)\n",
    "Y = y_array[1:].transpose(1,0,2)\n",
    "print(f'X.shape = {X.shape},\\t (K, T-1, S) = ({K}, {T-1}, {S})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I feel like we should be taking an average over the $K$ trials, for fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13115/1739891240.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  coll_P_cls = np.array([np.linalg.lstsq(X[k],Y[k])[0] for k in range(K-1)])\n"
     ]
    }
   ],
   "source": [
    "coll_P_cls = np.array([np.linalg.lstsq(X[k],Y[k])[0] for k in range(K-1)])\n",
    "P_CLS = coll_P_cls.mean(axis=0)\n",
    "P_CLS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5937108366452972"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(P_CLS-P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HLT-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
