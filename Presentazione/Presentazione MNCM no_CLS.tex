\documentclass[10pt,xcolor={table,dvipsnames}]{beamer} 		% carica automaticamente amsthm, amssymb, amsmath, graphicx
\setbeamertemplate{theorems}[numbered]%[ams style] 


\usepackage{appendixnumberbeamer}
\usepackage[T1]{fontenc}				% codifica dei font
\usepackage[utf8]{inputenc}				% lettere accentate da tastiera
\usepackage[italian]{babel}				% lingua del documento
\usepackage[italian]{varioref}			% Per usare il comando \vref{label}, che dà dei collegamenti più dettagliati

% Load the custom style file
\usepackage{AndreaStyle}
% The file `AndreaStyle.sty` is stored in: `D:\Programmi e Applicazioni\texlive\texmf-local\tex\latex\local` for Windows.
% The file `AndreaStyle.sty` is stored in: `/usr/local/texlive/texmf-local/tex/latex/local` for Ubuntu (desktop).
% This won't work in Overleaf, until the AndreaStyle.sty file is added to the project

\usepackage{mathdots}

%\usepackage{algorithm}
%\usepackage[beginLComment=//~,endLComment=~]{algpseudocodex}			% Package for typesetting algorithms

\usepackage{mathrsfs}					% Per dei caratteri matematici migliori: \mathscr{} e \mathcal{}
%\usepackage{braket} 					% Per il comando \Set, e altre (poche) cose
%\usepackage{textcomp}					% Dovrebbe aggiungere più simboli
\usepackage{bbm}						% Più simboli in \mathbb

%\usepackage{arydshln}					% per le linee tratteggiate nelle tabelle

%\usepackage[rightcaption]{sidecap}		% Per mettere le didascalie di lato

\usepackage{fontawesome5}				% Aggiunge simboli da FontAwesome

\usepackage{hyperref}					% Importante: hyperref va caricato nel documento.


%\setcounter{tocdepth}{1}	% profondità dell'indice

	% TEOREMI CUSTOM:
\theoremstyle{plain}					% Definisce ambienti per Teoremi, esercizi, corollari... Con lo stile adeguato
	\newtheorem{proposizione}{Proposizione}%[section]
	\newtheorem*{proposizione*}{Proposizione}
	
	\newtheorem{teorema}{Teorema}%[section]
	\newtheorem*{teorema*}{Teorema}
		
	%\newtheorem{lemma_es}{Lemma}[esercizio]
	%\newtheorem{lemma}{Lemma}[section]
	\newtheorem*{lemma*}{Lemma}
	\newtheorem{corollario}{Corollario}[section]


\theoremstyle{definition}				
	\newtheorem{definizione}{Definizione}%[section]%[chapter]
	\newtheorem*{definizione*}{Definizione}	%definizione non numerata
	\newtheorem*{notazione}{Notazione}

\theoremstyle{remark}
	\newtheorem{oss}{Osservazione}%[section]
	\newtheorem*{oss*}{Osservazione}


	% COMANDI CUSTOM
% Define the \indicator command
\NewDocumentCommand{\indicator}{O{t} O{m} O{i}}{%
  \mathlarger{\mathbbm{1}}\qty{\scriptstyle {x}_{#1}^{#2}=#3}%
}
% Define the \transpose command
%%%%%\newcommand{\transpose}[1]{\prescript{t}{}{#1}}
\newcommand{\transpose}[1]{#1^{\top}}
% Define the \Var command, for the variance
\newcommand{\Var}[1]{\operatorname{Var}\qty(#1)}
% Define the \Cov command, for the covariance
\newcommand{\Cov}[1]{\operatorname{Cov}\qty(#1)}

% Define a command to create unnumbered footnotes
\let\svthefootnote\thefootnote
\textheight 1in
\newcommand\blankfootnote[1]{%
  \let\thefootnote\relax\footnotetext{#1}%
  \let\thefootnote\svthefootnote%
}
% Define the \independent symbol, for independence
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
    \def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

%% Rename keywords for algorithms
%\algrenewcommand\algorithmicrequire{\textbf{Input:}}
%\algrenewcommand\algorithmicensure{\textbf{Output:}}





	%COLORI
\definecolor{madridlightblue}{RGB}{233, 233, 243}
	
\setbeamertemplate{page number in head/foot}[appendixframenumber]
	
% ------------------------- INIZIO CODICE -------------------------
\usetheme{Madrid}


\title[Seminario MNCM]{Consistently Estimating Markov Chains with Noisy Aggregate Data}			%WIP
%\subtitle{Presentazione e dimostrazione della convergenza} 
\author{Andrea Marino}
\institute[DI UniPi]{Università di Pisa}
%\titlegraphic{\includegraphics[width=2cm]{Immagini/cherubino_black.eps}}
\date[\today]{Metodi Numerici per le Catene di Markov\newline Seminario di fine corso}

%\AtBeginSubsection[] 						
%{
%	\begin{frame}
%		\frametitle{Sommario}
%		\tableofcontents[currentsection,subsectionstyle=show/shaded/hide] 
%	\end{frame}
%}

%% ----------- ALTERNATIVA -----------
% Custom command to insert the summary frame
\newcommand{\insertSummaryFrame}{
    \begin{frame}
        \frametitle{Sommario}
        \tableofcontents[currentsection, subsectionstyle=show/shaded/hide]
    \end{frame}
}

% Show summary at the beginning of each section
\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Sommario}
        \tableofcontents[currentsection, subsectionstyle=show/hide/hide]
    \end{frame}
}

% Show summary at the beginning of each subsection
\AtBeginSubsection[]
{
    \begin{frame}
        \frametitle{Sommario}
        \tableofcontents[currentsection, subsectionstyle=show/shaded/hide]
    \end{frame}
}


\begin{document}
	\begin{frame}[plain]
		\titlepage
	\end{frame}
	
\section*{Sommario}
	\setcounter{tocdepth}{1}
	\begin{frame}
		\frametitle{Sommario}
		\tableofcontents
	\end{frame}
	
	\setcounter{tocdepth}{2}



\section{Introduzione, notazione e prime definizioni}
	\subsection{Descrizione del problema}

	\begin{frame}{Introduzione al problema, notazione, modello 1/2}%{Descrizione del contesto}
		Supponiamo di avere una popolazione di $N\in\mathbb{N}_{>0}$ individui che evolvono da 
		uno stato all'altro {\smaller ($S\in\mathbb{N}_{>0}$: numero di possibili stati)}, 
		\emph{indipendentemente}, per $T\in\mathbb{N}_{>0}$ istanti temporali 
		{\smaller ($[k]\coloneqq\qty{1,\dots,k}$)}:
		\[
			\qty{x_{t}^{(m)}}_{t\in[T]}\sim\mathrm{Markov}\qty(\pi_0,P)\qquad\forall\,m\in[N].
		\]
		%\vspace*{-\baselineskip}
		%\begin{itemize}
		%	%\item<2-> $N$: dimensione della popolazione, $T$: istanti temporali {\smaller(sarà $T\to\infty$)}
		%	\item<2-> $S\in\mathbb{N}_{>0}$: numero di possibili stati
		%	\item<3-> $P\in\R{S}{S}$: matrice di transizione, $\pi_0\in\R{S}$: distribuzione iniziale
		%\end{itemize}
		%\onslide<2->{$S\in\mathbb{N}_{>0}$: numero di possibili stati, 
		%$P\in\R{S}{S}$: matrice di transizione, 
		%$\pi_0\in\R{S}$: distribuzione iniziale,
		%$\pi\in\R{S}$: distribuzione invariante.}
		%\smallskip 
		
		\onslide<2->{Supponiamo inoltre che:}
		\begin{itemize}
			\item<2-> La catena sia \emph{ergodica} e omogenea nel tempo
			(cfr. Appendice~\hyperlink{frame:catena_ergodica:appendice}{\faHandPointRight}).

			$\pi\in\R{S}$: distribuzione invariante 
			\item<3-> $x_t^{(m)}$ non sia osservabile per alcun $t\in[T], m\in[N]$, 
			ma che vi sia una \emph{conta aggregata} $\vb*{n}_t\in\R{S}$, definita t.c.
			\vspace*{-0.5\baselineskip}
			\[
				\vb*{n}_t(i)\coloneqq\sum_{m=1}^N\indicator
			\]
			\vspace*{-0.9\baselineskip}
			%$\vb*{n}_t(i)\coloneqq\sum_{m=1}^N\indicator$
			\item<4-> I dati osservati siano $\qty{\vb*{y}_1,\dots,\vb*{y}_T}$, 
			$\vb*{y}_t$ è ottenuto 
			da $\vb*{n}_t$ tramite un modello del rumore $\P{\vb*{y}_t}{\vb*{n}_t}$
			\item<5-> La raccolta è ripetuta $K$ volte, restituendo 
			$\qty{\vb*{y}_1^{(k)},\dots,\vb*{y}_T^{(k)}}_{k\in[K]}$
		\end{itemize}
	\end{frame}

	\begin{frame}{Introduzione al problema, notazione, modello 2/2}
		\begin{figure}[ht]
			\centering
			\includegraphics[width=0.45\textwidth]{Immagini/plate_model.png}
			\caption{\emph{Rappresentazione del modello in plate notation}}
		\end{figure}

		\vspace*{-0.5\baselineskip}
		La plate notation rappresenta graficamente come fattorizzare la legge congiunta:
		\begin{itemize}
			\item Cerchio vuoto: v.a. non osservata. Cerchio pieno: v.a. osservata
			\item<2-> Freccia: la v.a. in coda d'arco "influenza" la v.a. in testa
			\item<3-> Rettangolo: copia il contenuto tante volte quanto specificato dall'indice
		\end{itemize}

		%RIMANEGGIARE
		\begin{alertblock}<4->{Obiettivo}
			Usando le osservazioni
			$\big\{\vb*{y}_1^{(k)},\dots,\vb*{y}_T^{(k)}\big\}_{k\in [K]}$, stimare $P$.

			In particolare, vedremo come stimare $P$ usando il \emph{metodo dei momenti}.
			Lo stimatore dei momenti è consistente. 
		\end{alertblock}

	\end{frame}


	\subsection{Osservazioni su \texorpdfstring{$\vb*{n}_t$}{n_t} e su \texorpdfstring{$P$}{P}}

	\begin{frame}
		{\hypertarget{frame:dettagli_nt}{Osservazioni su $\vb*{n}_t$ e su $P$}}
		\begin{definizione*}
			Siano $\vb*{\mu}_t\in\R{S}$ e $\vb*{\mu}_{t,t+1}\in\R{S}{S}$ il vettore e la matrice
			delle leggi marginali:
			\vspace*{-0.5\baselineskip}
			\[\begin{aligned}
				\vb*{\mu}_t(i)=\P{x_t=i}=\qty(\transpose{\pi_0}P^t)_i & \qquad & \vb*{\mu}_{t,t+1}(i,j)=\P{x_t=i,x_{t+1}=j}\\
			\end{aligned}\]
		\end{definizione*}

		\begin{oss*}<2->
			\begin{itemize}
				\item<2-> $\vb*{n}_t(i)=\sum_{m=1}^N\indicator$: numero di individui nello stato $i\in S$,
				al tempo $t\in T$.
				\item<3-> $\P{\indicator =1}=\P{x_t^{(m)}=i}=\vb*{\mu}_t(i)$
				\item<4-> $\vb*{n}_t$ è multinomiale di parametri $N,\vb*{\mu}_t$
				(cfr. Appendice~\hyperlink{frame:dettagli_nt:appendice}{\faHandPointRight})
			\end{itemize}
		\end{oss*}

		\begin{oss*}<5->
			Si ha $P=\operatorname{Diag}\qty(\vb*{\mu}_t)^{-1}\cdot\vb*{\mu}_{t,t+1}$.

			\onslide<6->{Se $\pi_0=\pi$, $\vb*{\mu}_t=\pi\quad\forall\,t\in[T]$ e dunque 
			$\operatorname{Diag}\qty(\vb*{\mu}_t)$ è invertibile.} 
			\onslide<7->{Altrimenti, $\vb*{\mu}_t>0$ per $t$ grande abbastanza. }
			%infatti 
			%$P_{i,j}=\P{x_{t+1}=j}{x_t=i}=\P{x_t=i,x_{t+1}=j}/\,\P{x_t=i}=\vb*{\mu}_{t,t+1}(i,j)/\,\vb*{\mu}_t(i)$
		\end{oss*}

	\end{frame}



\section{Stimare \texorpdfstring{$P$}{P} con il metodo dei momenti}
	
	\begin{frame}
		{\hypertarget{frame:prop_1}{Momenti primi e secondi}}
		
		\setbeamercovered{transparent}
		\begin{notazione}
			Denotiamo \onslide<1,3->{il momento primo}\onslide<3->{ e} 
			\onslide<2,3->{i momenti secondi centrali} con:
			%\vspace{-0.5\baselineskip}
            \begin{align*}
                \onslide<1,3->{\R{S}\ni\,&\vb*{m}_t\coloneqq\mathbb{E}\qty[\vb*{n}_t]} & 
                \onslide<2,3->{\R{S}{S}\ni\,&\Sigma_t\coloneqq\Var{\vb*{n}_t}} & 
                \onslide<2,3->{\R{S}{S}\ni\,&\Sigma_{t,t+1}\coloneqq\Cov{\vb*{n}_t,\vb*{n}_{t+1}}}\\
            \end{align*}
		\end{notazione}
		\setbeamercovered{invisible}

		\begin{proposizione}<4->\label{prop:formule_momenti}
			\vspace{-0.25\baselineskip}
			Per ogni $t\in[T]$, si ha:
			\vspace{-0.25\baselineskip}
			\begin{columns}
				\begin{column}{0.45\textwidth}
					\begin{enumerate}
						\item $\vb*{m}_t=N\vb*{\mu}_t$
					\end{enumerate}
				\end{column}
				\begin{column}{0.55\textwidth}
					\begin{enumerate}
						\setcounter{enumi}{1}
                        \item $\Sigma_t=N\cdot\qty(\operatorname{Diag}(\vb*{\mu}_t)-\vb*{\mu}_t\transpose{\vb*{\mu}_t})$	
                        \item $\Sigma_{t,t+1}=N\cdot\qty(\vb*{\mu}_{t,t+1}-\vb*{\mu}_t\transpose{\vb*{\mu}_{t+1}})$
					\end{enumerate}
				\end{column}				
			\end{columns}
		\end{proposizione}
		\begin{proof}<5->
			Cfr. Appendice~\hyperlink{frame:dim_prop_1:appendice}{\faHandPointRight}
		\end{proof}
	\end{frame}

	\begin{frame}
		{Lo stimatore dei momenti}
		Ricordiamo che $P=\operatorname{Diag}\qty(\vb*{\mu}_t)^{-1}\cdot\vb*{\mu}_{t,t+1}$.
		\onslide<2->{Lo stimatore dei momenti si ottiene sostituendo a 
		$\vb*{\mu}_t,\vb*{\mu}_{t,t+1}$ le loro stime empiriche:} 
        \begin{itemize}
            \item<2-> $\mathbf{\vb*{\mu}_t}:$ Dalla proposizione~\ref{prop:formule_momenti} {\smaller (e poiché $\vb*{\mu}_t$ è un vettore stocastico)} segue che $\vb*{\mu}_t=\frac{\vb*{m}_t}{\norm{\vb*{m}_t}_1}$. 
			\onslide<3->{Dunque 
            \[
                \hat{\vb*{\mu}}_t=\frac{\hat{\vb*{m}}_t}{\norm{\hat{\vb*{m}}_t}_1}
            \]}
            \onslide<4->{dove $\hat{\vb*{m}}_t\coloneqq\frac{1}{K}\sum_{k=1}^K\vb*{y}_t^{(k)}$ è 
			il valore atteso empirico.}
            \item<5-> $\mathbf{\vb*{\mu}_{t,t+1}}:$ La proposizione~\ref{prop:formule_momenti} suggerisce
            \[
                \hat{\vb*{\mu}}_{t,t+1}=\frac{1}{N}\hat{\Sigma}_{t,t+1}+\hat{\vb*{\mu}}_t\transpose{\hat{\vb*{\mu}}_{t+1}},
            \]
            \onslide<6->{dove 
            $\hat{\Sigma}_{t,t+1}\coloneqq\frac{1}{K}\sum_{k=1}^K\mathsmaller{ \qty(\vb*{y}_t^{(k)}-\hat{\vb*{m}}_t)\cdot\transpose{\qty(\vb*{y}_{t+1}^{(k)}-\hat{\vb*{m}}_{t+1})}}$ è la covarianza empirica.}
        \end{itemize}
        \onslide<7->{Otteniamo dunque $\hat{P}_{\textup{MoM}}\coloneqq\operatorname{Diag}\qty(\hat{\vb*{\mu}}_t)^{-1}\qty(N^{-1}\hat{\Sigma}_{t,t+1}+\hat{\vb*{\mu}}_t\transpose{\hat{\vb*{\mu}}_{t+1}})$}
	\end{frame}

	\begin{frame}
		{\hypertarget{frame:prop_noise_model}{Modelli di rumore 1/2}}{Ricavare i momenti di $\vb*{n}$ da quelli di $\vb*{y}$}
		Per poter usare con successo $\hat{P}_{\textup{MoM}}$ con osservazioni rumorose,
		dobbiamo poter ricavare i momenti di $\vb*{n}$ a partire dai momenti di $\vb*{y}$.

		%Perché...
		
		%\onslide<2->{La proposizione~\ref{prop:noise_model} delinea un'ampia classe di modelli 
		%di rumore che permettono di fare ciò:}
		\begin{proposizione}<2->\label{prop:noise_model}
			Supponiamo che il modello di rumore $\P{\vb*{y}}{\vb*{n}}$ soddisfi le seguenti
			due condizioni:
			\begin{enumerate}
				\item<2-> $\vb*{y}_t\independent\vb*{y}_s\qquad\forall\,t\ne s\in[T]\quad$ {\smaller (rumore indipendente)}
				\item<3-> $\mathbb{E}\qty[\vb*{y}_t\mid\vb*{n}_t]=A_t\vb*{n}_t,\quad$ dove $A_t\in\R{S}{S}$ è una 
				matrice \emph{invertibile} nota, $\forall t\in[T]$.
			\end{enumerate}
			\onslide<4->{Allora valgono le seguenti relazioni:}
			\begin{enumerate}
				\item<5-> $\mathbb{E}\qty[\vb*{n}_t]=A_t^{-1}\cdot\mathbb{E}\qty[\vb*{y}_t]\qquad\forall\,t\in[T]$
				\item<5-> $\mathbb{E}\qty[\vb*{n}_s\transpose{\vb*{n}_t}]=A_s^{-1}\cdot\mathbb{E}\qty[\vb*{y}_s\transpose{\vb*{y}_t}]\cdot{A_t^{-\top}}\qquad\forall\,s\ne t\in[T]$
				\item<5-> $\Cov{\vb*{n}_s,\vb*{n}_t}=A_s^{-1}\cdot\Cov{\vb*{y}_s,\vb*{y}_t}\cdot{A_t^{-\top}}\qquad\forall\,s\ne t\in[T]$
			\end{enumerate}
		\end{proposizione}
		\begin{proof}<6->
			Cfr. Appendice~\hyperlink{frame:dim_prop_noise_model:appendice}{\faHandPointRight}
		\end{proof}
	\end{frame}

    \begin{frame}
        {Modelli di rumore 2/2}{Esempi di modelli di rumore}
        Vediamo esempi di modelli di rumore che rispettano la proposizione~\ref{prop:noise_model}:
        
        \begin{itemize}
            \item<1-> \textbf{Rumore additivo}: $\vb*{y}_t=\vb*{n}_t+\epsilon_t$ con $\epsilon_t\sim\mathcal{N}\qty(0,\sigma^2\cdot I)$ e $\epsilon_t\independent\epsilon_s\quad\forall\,t\ne s$.\newline
            \onslide<2->{In questo caso $A_t=I\quad\forall\,t\in[T]$.}
            \hspace*{.025\linewidth}\begin{minipage}{.95\linewidth}
                \onslide<3->{\begin{block}{Idea}
                    Aggiunta artificiale di rumore ai dati originali, per garantire la privacy degli membri della popolazione. \emph{Differential privacy}. 
                \end{block}}
            \end{minipage}
            \item<4-> \textbf{Rumore binomiale} $\P{\vb*{y}_t(i)}{\vb*{n}_t(i)}=\operatorname{Binomial}\qty(\vb*{n}_t(i),\alpha_i)$, $\alpha_i\in(0,1)$, $\forall\,i\in[S]$.\newline
            \onslide<5->{In questo caso, $\mathbb{E}\qty[\vb*{y}_t(i)\mid\vb*{n}_t(i)]=\alpha_i\vb*{n}_t(i)$, dunque $A_t=\operatorname{Diag}{\qty(\alpha_1,\dots,\alpha_S)}\quad\forall\,t\in[T]$.}
            \hspace*{.025\linewidth}\begin{minipage}{.95\linewidth}
                \onslide<6->{\begin{block}{Idea}
                    Errore nella misurazione. I membri della popolazione che si trovano nello stato $i\in[S]$ sono osservati (ossia "contati") indipendentemente, con probabilità $\alpha_i$.
                \end{block}}
            \end{minipage}
        \end{itemize}
        
    \end{frame}



\section{Algoritmo e analisi teorica}
    \subsection{Algoritmi per il calcolo di \texorpdfstring{$\hat{P}_{\textup{MoM}}$}{P}}

	\begin{frame}[fragile]
		{Algoritmo per il calcolo di $\hat{P}_{\textup{MoM}}$ {\smaller (caso non stazionario)}}

		\begin{figure}[ht]
			\centering
			\includegraphics[width=0.65\textwidth]{Immagini/algorithm_nonstationary.png}
			%\caption{\emph{Algoritmo per il calcolo di $\hat{P}_{\textup{MoM}}$}}
		\end{figure}
		
	\end{frame}

	\begin{frame}[fragile]
		{Algoritmo per il calcolo di $\hat{P}_{\textup{MoM}}$ {\smaller (caso stazionario)}}
		Caso {\smaller (fortemente)} stazionario: $\pi_0=\pi$.

		\begin{figure}[ht]
			\centering
			\includegraphics[width=0.65\textwidth]{Immagini/algorithm_stationary.png}
			%\caption{\emph{Algoritmo per il calcolo di $\hat{P}_{\textup{MoM}}$}}
		\end{figure}
		
	\end{frame}

    \begin{frame}
        {\hypertarget{frame:P_mom_stochastic}{Osservazioni su $\hat{P}_{\textup{MoM}}$}}
        
        In alcuni casi, per effetto del rumore, $\hat{\vb*{m}}_t=\frac{1}{K}\sum_{k=1}^K\vb*{y}_t^{(k)}$ potrebbe essere negativo. \onslide<2->{Dunque $\hat{P}_{\textup{MoM}}$ potrebbe avere elementi negativi.}
        \smallskip 
        
        \onslide<3->{Nel caso $\hat{\vb*{m}}\ge 0$, si ha (cfr. Appendice~\hyperlink{frame:P_mom_stochastic:appendice}{\faHandPointRight}) $\hat{P}_{\textup{MoM}}\vb*{e}=\vb*{e}\iff\hat{\Sigma}\vb*{e}=0$.} 
    
        \onslide<4->{Supponendo per semplicità $K=1$ e rumore gaussiano $\vb*{y}_t=\vb*{n}_t+\epsilon_t$, si può ricavare
        \[
            \hat{\Sigma}\vb*{e}=0\iff\sum_{t=1}^{T-1}\langle r_{t+1},\vb*{e}\rangle\cdot r_t=0\iff 
            \begin{multlined}[t]
                \langle\vb*{m}_{\epsilon},\vb*{e}\rangle\cdot\big((T+1)\hat{\vb*{m}}-\vb*{y}_T\big)+\\+\langle\epsilon_1,\vb*{e}\rangle\cdot\hat{\vb*{m}}+\sum_{t=1}^{T-1}\langle\epsilon_{t+1},\vb*{e}\rangle\cdot\vb*{y}_t=0
            \end{multlined}
        \]
        dove $r_t=\vb*{y}_{t}-\hat{\vb*{m}}$; e $\vb*{m}_{\epsilon}=\frac{1}{T}\sum_{t=1}^T\epsilon_t$ è tale che $\hat{\vb*{m}}=\vb*{m}_{\epsilon}+\frac{1}{T}\sum_{t=1}^T\vb*{n}_t$.}

        \onslide<5->{Dunque in assenza di rumore $\hat{\Sigma}\vb*{e}=0$, ma in generale $\hat{P}_{\textup{MoM}}$ \emph{non} è stocastica.}

        \onslide<6->{
        \begin{oss*}
            Si può fare qualche considerazione sui termini di rumore $\langle\vb*{m}_{\epsilon},\vb*{e}\rangle$ e $\langle\epsilon_t,\vb*{e}\rangle$, cfr. Appendice~\hyperlink{frame:P_mom_stochastic_noise:appendice}{\faHandPointRight}.
        \end{oss*}}
        
    \end{frame}


	\subsection{Dimostrazione della consistenza di \texorpdfstring{$\hat{P}_{\textup{MoM}}$}{P_mom}}

	\begin{frame}
        {\hypertarget{frame:teorema_1_part1}{$\hat{P}_{\textup{MoM}}$ è consistente$\qquad 1/3$}}

        Supponiamo che:
        \begin{enumerate}
            \item $\pi_0=\pi$, ossia $\qty{x_t^{(m)}}_{t\in[T]}$ è \emph{fortemente} stazionario
            \item<2-> $\P{\vb*{y}}{\vb*{n}}$ rispetti le ipotesi della proposizione~\ref{prop:noise_model}
            \item<3-> $N\in\mathbb{N}, A\in\R{S}{S}$ siano noti {\smaller (e costanti: forte stazionarietà $\implies A_t=A\quad\forall\,t\in[T]$)}
        \end{enumerate}

        \begin{teorema}<4->\label{teor:consistenza_P_mom}
            Sia $\hat{P}_{T,K}$ lo stimatore restituito dall'algoritmo per il caso stazionario.

            Nelle ipotesi precedenti, $\hat{P}_{T,K}$ è consistente.%, ossia converge in probabilità 
            %a $P$ per $T\to\infty$ e/o $K\to\infty$.
        \end{teorema}
        \begin{block}<5->{Dimostrazione.}
            \begin{itemize}
                \item ${K\to\infty}$
                \begin{itemize}
                    \item<5-> Per la legge dei grandi numeri, $\lim_K\hat{\vb*{m}}_t=\mathbb{E}\qty[\vb*{y}_t]$ q.c.
                    \onslide<6->{Dunque 
                    $\lim_K\hat{\vb*{m}}=\lim_k\frac{1}{T}\sum_{t=1}^T\hat{\vb*{m}}_t=\mathbb{E}\qty[\vb*{y}_1]$ q.c.
                    (cfr. Appendice~\hyperlink{frame:teorema1_lim_mhat:appendice}{\faHandPointRight})}
                    \item<7-> Similmente si dimostra che $\lim_K\hat{\Sigma}_{t,t+1}=\Cov{\vb*{y}_1,\vb*{y}_2}$ in prob.,
                    e dunque $\hat{\Sigma}=\frac{1}{T-1}\sum_{t=1}^{T-1}\hat{\Sigma}_{t,t+1}\overset{K\to\infty}{\longrightarrow}\Cov{\vb*{y}_1,\vb*{y}_2}$ in prob.
                    (cfr. Appendice~\hyperlink{frame:teorema1_lim_Sigmahat:appendice}{\faHandPointRight})
                \end{itemize}
                \onslide<8->{Ma allora $\hat{P}_{T,K}\xrightarrow{K\to\infty}P$ in probabilità.}
            \end{itemize}
        \end{block}
    \end{frame}

    \begin{frame}
        {\hypertarget{frame:teorema_1_part2}{$\hat{P}_{\textup{MoM}}$ è consistente$\qquad 2/3$}}

        \begin{block}{}
            \begin{itemize}
                \item $T\to\infty$
                
                Possiamo supporre $K=1$. Dunque $\hat{\vb*{m}}_t=\vb*{y}_t$ e $\hat{\vb*{m}}=\frac{1}{T}\sum_{t=1}^{T}\vb*{y}_t$. \onslide<2->{Dobbiamo dimostrare che (in probabilità)
                \begin{gather*}
                    \hat{\vb*{m}}\xrightarrow{T\to\infty}\mathbb{E}\qty[\vb*{y}_1],\text{ e }\\
                    \hat{\Sigma}\coloneqq\frac{1}{T-1}\sum_{t=1}^{T-1}\qty(\vb*{y}_t-\hat{\vb*{m}})\cdot\transpose{\qty(\vb*{y}_{t+1}-\hat{\vb*{m}})}\xrightarrow{T\to\infty}\Cov{\vb*{y}_1,\vb*{y}_2}
                \end{gather*}}
                %in probabilità.
                \vspace{-\baselineskip}
                
                \hspace*{.05\linewidth}\begin{minipage}{.9\linewidth}
                    \begin{definizione*}<3->
                        $\qty{X_t}_{t\in[T]}$ {\smaller (fortemente stazionario)} è detto \emph{mean-ergodic} se $\frac{1}{T}\sum_{t=1}^TX_t$ converge in media seconda alla media della popolazione $\mathbb{E}\qty[X_t]$. 
                    \end{definizione*}
                    \begin{oss*}<4->
                        Convergenza in media seconda $\implies$ convergenza in probabilità.
                    \end{oss*}
                \end{minipage}
            \end{itemize}
        \end{block}

    \end{frame}

    \begin{frame}
        {\hypertarget{frame:teorema_1_part3}{$\hat{P}_{\textup{MoM}}$ è consistente$\qquad 3/3$}}

        \begin{block}{}
            ${\vb*{y}}_t$ 
            mean-ergodic vuol dire che $\frac{1}{T}\sum_{t=1}^T\vb*{y}_t\xrightarrow{T\to\infty}\mathbb{E}\qty[\vb*{y}_1]$.
            \onslide<2->{Similmente, $\hat{\Sigma}\xrightarrow{T\to\infty}\Cov{\vb*{y}_1,\vb*{y}_2}$ se il processo $\qty{\vb*{Z}_t}_{t\in[T-1]}$, $\vb*{Z}_t\coloneqq\vb*{y}_t\cdot\transpose{\vb*{y}_{t+1}}$, è mean-ergodic.}

            \onslide<3->{Per dimostrarlo, è sufficiente dimostrare che $\qty{\vb*{Z}_t(i,j)}_{t\in[T-1]}$ è 
            mean-ergodic $\quad$ ({\smaller $\vb*{Z}_t(i,j)=\vb*{y}_{t}(i)\vb*{y}_{t+1}(j)$}).}
            
            \hspace{.05\linewidth}\begin{minipage}{.9\linewidth}
                \begin{block}<4->{Proposizione {\smaller~\cite[Theorem 12.2 (p.~528)]{book:Papoulis}}}
                    Sia $\qty{X_t}_{t\in[T]}$ stazionario e sia $\gamma_X(\tau)\coloneqq\Cov{X_{t+\tau},X_t}$ la sua funzione di autocovarianza. Se $\lim_{\tau\to\infty}\gamma_X(\tau)=0$ allora $\qty{X_t}_{t\in[T]}$ è mean-ergodic. 
                \end{block}                
            \end{minipage}
            \medskip
            
            \begin{enumerate}
                \item<5-> Consideriamo i processi $\qty{\vb*{n}_t}_{t\in[T]}$ e $\qty{z_t(i,j)}_{t\in[T-1]}$, $z_t(i,j)\coloneqq\vb*{n}_t(i)\vb*{n}_{t+1}(j)$ 
                \item<6-> Si dimostra che $\gamma_{\vb*{n}}(\tau)\xrightarrow{\tau\to\infty}0$ {\smaller (cfr. Appendice~\hyperlink{frame:teorema1_lim_gamma_n:appendice}{\faHandPointRight})} e che $\gamma_{z(i,j)}(\tau)\xrightarrow{\tau\to\infty}0$
                \item<7-> Si dimostra che $\qty{\vb*{n}_t}_{t\in[T]}$ mean-ergodic $\implies\qty{\vb*{y}_t}_{t\in[T-1]}$ mean-ergodic {\smaller (cfr. Appendice~\hyperlink{frame:teorema1_yt_tp1_mean_ergodic:appendice}{\faHandPointRight})}
                \item<8-> Si dimostra che $\qty{z_t(i,j)}_{t\in[T-1]}$ mean-ergodic $\implies\qty{\vb*{Z}_t(i,j)}_{t\in[T-1]}$ mean-ergodic {\smaller (cfr. Appendice~\hyperlink{frame:teorema1_yt_mean_ergodic:appendice}{\faHandPointRight})}.\onslide<8->{\hfill$\square$}
            \end{enumerate}
        \end{block}
        
    \end{frame}



\section{Esperimenti numerici}
    \begin{frame}{Rischio degli stimatori}
        \begin{teorema}
            Per $TK\to\infty$, gli stimatori $\hat{\vb*{m}}$ e $\hat{\Sigma}$ sono non distorti, e hanno varianza (dunque rischio quadratico) $O(1/TK)$. 
        \end{teorema}
        \begin{proof}<2->
            La dimostrazione si ottiene adattando quella del teorema~\ref{teor:consistenza_P_mom} e usando~\cite[12-12 (p.~528)]{book:Papoulis}. Maggiori dettagli in~\cite{article:main}.
        \end{proof}
        \medskip

        \onslide<3->{Un'analisi teorica del rischio di $\hat{P}_{\textup{MoM}}$ non è stata svolta in~\cite{article:main}.} 
        
        \onslide<4->{Gli autori hanno presentato risultati empirici che suggeriscono che anche l'errore quadratico medio di $\hat{P}_{\textup{MoM}}$ sia $O(1/TK)$.}

        \onslide<5->{L'errore è dato da $\frac{1}{S^2}\norm*{\hat{P}-P}_F^2$.         
        {\smaller (Per una discussione più approfondita, cfr.~\cite{article:main})}.}
    \end{frame}

    \begin{frame}
        {Esperimenti numerici}
        {MSE vs. \texorpdfstring{$T\times K$}{TxK}, algoritmo per il calcolo di $\hat{P}_{\textup{MoM}}$ {\smaller (caso stazionario)}}
        \begin{figure}[ht]
			\centering
			\includegraphics[width=\textwidth]{Immagini/mom_cls_results_authors.png}
			%\caption{\emph{MSE per la stima di $\hat{P}_{\textup{MoM}}$}, con errore}
		\end{figure}
        \begin{description}
            \item[(a)] $\vb*{y}_t(i)\mid\vb*{n}_t(i)\sim\mathrm{Binomial}\qty(\vb*{n}_t(i),\alpha)\quad\alpha\in[0,1]$
            \item[(b)] $\vb*{y}_t=\vb*{n}_t+\epsilon_t\quad\epsilon_t\sim\mathcal{N}\qty(0,\sigma^2\cdot I)$
        \end{description}

        $S=10, N=100$, $P_{i,:}\sim\operatorname{Dirichlet}\qty(\frac{0.5}{S}\vb*{e})\quad\forall\,i\in[S]$
    \end{frame}








	

    
\section{Ringraziamenti e bibliografia}
    \begin{frame}
        \begin{center}
            \Huge{Grazie per l'attenzione!}
        \end{center}
    \end{frame}

	\begin{frame}{\refname}
		\begin{thebibliography}{9}
			\bibitem{article:main} Garrett Bernstein, Daniel Sheldon
			\newblock Consistently Estimating Markov Chains with Noisy Aggregate Data
			\newblock Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, \emph{PMLR}, vol. 51, pp. 1142-1150, PMLR (2016) 09-11 May

			\bibitem{book:Norris} J. R. Norris
			\newblock Markov Chains
			\newblock Cambridge University Press, 1997

			\bibitem{book:Berger_Casella} R. L. Berger, G. Casella
			\newblock Statistical Inference (2nd ed.)
			\newblock Chapman and Hall/CRC, 2024

			\bibitem{book:Papoulis} A. Papoulis, S. U. Pillai
			\newblock Probability, Random Variables and Stochastic Processes (4th ed.)
			\newblock McGraw-Hill, 2002
		\end{thebibliography}
	\end{frame}

% ----------------------------------APPENDICE----------------------------------
\appendix

\section*{Appendice}
	\begin{frame}
		\begin{center}
			\Huge{\textbf{Appendice}}
		\end{center}
	\end{frame}

	\begin{frame}
		{\hypertarget{frame:catena_ergodica:appendice}{Catene di Markov ergodiche}}
		\begin{definizione*}
			Ricordiamo che una catena di Markov è detta \alert{ergodica} se è
			irriducibile, positiva ricorrente e aperiodica.
		\end{definizione*}
		\onslide<2->{Una catena ergodica {\smaller (in quanto irriducibile e positiva 
		ricorrente)} ha una distribuzione invariante $\pi$. Inoltre:}
		\begin{itemize}
			\item<2-> $\pi$ è unica
			\item<3-> $\pi_i=1/\,\mathbb{E}_i\qty[T_i]$ 
			{\smaller (ovvero 1/tempo atteso di ritorno)}, dove 
			$T_i\coloneqq\inf_{n\ge 1}\qty{X_n=i}$ {\smaller (istante di primo passaggio)},
			e in particolare $\pi_i>0$			
		\end{itemize}
		\begin{oss*}<4->
			Per una catena finita e irriducibile {\smaller (dunque positiva ricorrente)},
			le due condizioni precedenti si possono ricavare dal teorema di Perron-Frobenius.
		\end{oss*}

		\onslide<5->{Il \emph{teorema di convergenza all'equilibrio} dice che per una 
		catena ergodica vale $\lim_{n\to\infty}\P{x_n=i}=\pi_i$. 
		In particolare $P_{i,j}\overset{n}{\longrightarrow}\pi_j\quad\forall\,i,j\in[S]$.}

		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:intro}{\faHandPointLeft}}
	\end{frame}

	\begin{frame}
		{\hypertarget{frame:dettagli_nt:appendice}{Dettagli sulla legge di $\vb*{n}_t$}}		
		$\R{S}\ni\vb*{n}_t$, e $\vb*{n}_t(i)=\sum_{m=1}^N\indicator$.
		\smallskip

		\onslide<2->{Possiamo interpretare $\vb*{n}_t$ come la somma di $N$ v.a. i.i.d.
		$V_t^{(1)},\dots,V_t^{(N)}$, ciascuna delle quali codifica lo stato 
		dell'individuo $m\in[N]$ corrispondente, al tempo $t\in[T]$.}

		\onslide<3->{$V_t^{(m)}$ rappresenta il risultato di una "prova" con $S$ 
		possibili \emph{outcomes}. L'outcome $i\in [S]$ è rappresentato dal versore 
		canonico $e_i\in\R{S}$.} 
		\onslide<4->{Si ha: 
		\[
			\P{V_t^{(m)}=e_i}=\P{x_t^{(m)}=i}=\vb*{\mu}_t(i)
		\]
		dunque $V_t^{(m)}$ è \emph{categorica} di parametro $\vb*{\mu}_t$, 
		$\forall\,m\in[N]$.} 
		\onslide<5->{Di conseguenza $\vb*{n}_t$ è multinomiale di 
		parametri $N,\vb*{\mu}_t$.}

		%\textcolor{red}{Disegno}
		\onslide<6->{\begin{figure}[ht]
			\centering
			\includegraphics[width=0.5\textwidth]{Immagini/Disegno.png}
			%\caption{\emph{Algoritmo per il calcolo di $\hat{P}_{\textup{MoM}}$}}
		\end{figure}}
		
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:dettagli_nt}{\faHandPointLeft}}
	\end{frame}

	\begin{frame}
		{\hypertarget{frame:dim_prop_1:appendice}{Dimostrazione della Proposizione~\ref{prop:formule_momenti}$\qquad$ 1/4}}
		\begin{block}{Dimostrazione.}
			$\vb*{n}_t$ è multinomiale di parametri $N$ e $\vb*{\mu}_t$, 
			$\vb*{n}_t=\sum_{m=1}^{N}V_t^{(m)}$, con $V_t^{(m)}$ categorica di parametro $\vb*{\mu}_t$,
			e $V_t^{(m)}\independent V_t^{(m')}\quad\forall m\ne m'$. Ometteremo l'apice quando sarà irrilevante.
			\begin{enumerate}
				\item\label{proof:prop_1:point_1} $\mathbb{E}\qty[\vb*{n}_t]=N\vb*{\mu}_t$ è un risultato noto sulle v.a. multinomiali. 
				Segue dal fatto che $\vb*{n}_t(i)$ è binomiale di parametri $N$ e $\vb*{\mu}_t(i)$. 
				\smallskip

				Comunque,
				$\mathbb{E}\qty[\vb*{n}_t]=\sum_{m=1}^{N}\mathbb{E}\qty[V_t^{(m)}]=N\,\mathbb{E}\qty[V_t]$, e si ha
				\vspace{-0.5\baselineskip}
				\begin{align*}
					\R{S}\ni\mathbb{E}\qty[V_t]&=\sum_{i=1}^{S}e_i\cdot\P{V_t=e_i}=\sum_{i=1}^{S}e_i\cdot\P{V_t(i)=1}\\
					& = \sum_{i=1}^{S}e_i\cdot\P{\indicator[t][{}][i]=1}=\sum_{i=1}^{S}e_i\cdot\vb*{\mu}_t(i)=\vb*{\mu}_t
				\end{align*}
				da cui segue che $\vb*{m}_t=\mathbb{E}\qty[\vb*{n}_t]=N\,\vb*{\mu}_t$.				
			\end{enumerate}
		\end{block}
	
		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:prop_1}{\faHandPointLeft}}
	\end{frame}

	\begin{frame}
		{Dimostrazione della Proposizione~\ref{prop:formule_momenti}$\qquad$ 2/4}
		
		\begin{block}{}
			\begin{enumerate}
				\setcounter{enumi}{1}
				\item Per indipendenza e per il punto~\eqref{proof:prop_1:point_1} si ha
				\vspace{-0.5\baselineskip}
				\begin{align*}
					\Sigma_t&=\Var{\vb*{n}_t}=\sum_{m=1}^{N}\Var{V_t^{(m)}}=N\cdot\Var{V_t}\\
					&=N\cdot(\mathbb{E}\qty[V_t\transpose{V_t}]-\mathbb{E}\qty[V_t]\cdot\mathbb{E}\qty[\transpose{V_t}])=N\cdot(\mathbb{E}\qty[V_t\transpose{V_t}]-\vb*{\mu}_t\transpose{\vb*{\mu}_t})
				\end{align*}
				Ora, $\R{S}{S}\ni\mathbb{E}\qty[V_t\transpose{V_t}]$ è t.c.
				\vspace{-0.5\baselineskip}
				\begin{align*}
					\mathbb{E}\qty[V_t\transpose{V_t}]_{i,j}&=\mathbb{E}\qty[V_t(i)\cdot V_t(j)]=\P{V_t(i)\cdot V_t(j)=1}\\
					& = \P{V_t(i)=1,V_t(j)=1}=
					\begin{dcases*}
						0 & se $i\ne j$\\
						\vb*{\mu}_t(i) & se $i=j$
					\end{dcases*}
				%\intertext{$V_t\in\qty{e_1,\dots,e_S}$, quindi non può
				%assumere valore 1 su due componenti diverse. Ne segue che}
				%	&  \P{V_t(i)=1,V_t(j)=1}=
				%	\begin{dcases*}
				%		0 & se $i\ne j$\\
				%		\vb*{\mu}_t(i) & se $i=j$
				%	\end{dcases*}\\
				\end{align*}
				Infatti $V_t\in\qty{e_1,\dots,e_S}$, quindi non può
				assumere valore 1 su due componenti diverse.

				Dunque $\mathbb{E}\qty[V_t\transpose{V_t}]=\operatorname{Diag}\qty(\vb*{\mu}_t)$ 
				e si ha $\Sigma_t=N\cdot(\operatorname{Diag}\qty(\vb*{\mu}_t)-\vb*{\mu}_t\transpose{\vb*{\mu}_t})$
			\end{enumerate}
		\end{block}

		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:prop_1}{\faHandPointLeft}}
	\end{frame}

	\begin{frame}
		{Dimostrazione della Proposizione~\ref{prop:formule_momenti}$\qquad$ 3/4}
		
		\begin{block}{}
			\begin{enumerate}
				\setcounter{enumi}{2}
				\item Si ha 
				\vspace*{-\baselineskip}
				\begin{align*}
					\Sigma_{t,t+1}&=\Cov{\sum_{m=1}^N V_{t}^{(m)},\sum_{m'=1}^N V_{t+1}^{(m')}}=\smashoperator[lr]{\sum_{m,m'}}\Cov{V_t^{(m)},V_{t+1}^{(m')}}\\
					&=\sum_{m=m'}\Cov{V_t^{(m)},V_{t+1}^{(m)}}+\sum_{m\ne m'}\Cov{V_t^{(m)},V_{t+1}^{(m')}}=
				\intertext{per indipendenza si ha}
					&=\sum_{m=m'}\Cov{V_t^{(m)},V_{t+1}^{(m)}}+0=N\cdot\Cov{V_t,V_{t+1}} .
				\end{align*}
				Dunque $\Sigma_{t,t+1}=N\cdot\Cov{V_t,V_{t+1}}=N\cdot\qty(\mathbb{E}\qty[V_t\transpose{V_{t+1}}]-\vb*{\mu}_t\transpose{\vb*{\mu}_{t+1}}).$
				\smallskip

				Procedendo in maniera analoga al punto precedente si dimostra che 
				\vspace*{-0.25\baselineskip}
				\[
					\mathbb{E}\qty[V_t\transpose{V_{t+1}}]_{i,j}=\P{V_t(i)=1,V_{t+1}(j)=1}=\vb*{\mu}_{t,t+1}(i,j)
				\]
				%\vspace*{-0.5\baselineskip}
				da cui segue che $\Sigma_{t,t+1}=N\cdot\qty(\vb*{\mu}_{t,t+1}-\vb*{\mu}_t\transpose{\vb*{\mu}_{t+1}}).$
			\end{enumerate}			
		\end{block}

		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:prop_1}{\faHandPointLeft}}
	\end{frame}

	\begin{frame}
		{Dimostrazione della Proposizione~\ref{prop:formule_momenti}$\qquad$ 4/4}

		\begin{block}{}
			\begin{enumerate}
				\setcounter{enumi}{3}
				\item Si ha che $\Lambda_{t}=\mathbb{E}\qty[\vb*{n}_t\transpose{\vb*{n}_t}]$ per definizione,
				ma 
				\begin{align*}
					\mathbb{E}\qty[\vb*{n}_t\transpose{\vb*{n}_t}]&=\Var{\vb*{n}_t}+\mathbb{E}\qty[\vb*{n}_t]\cdot\mathbb{E}\qty[\transpose{\vb*{n}_t}]\\
					&=\Sigma_t+N^2\cdot\vb*{\mu}_t\transpose{\vb*{\mu}_t}=N\cdot\qty(\operatorname{Diag}(\vb*{\mu}_t)+(N-1)\vb*{\mu}_t\transpose{\vb*{\mu}_t})
				\end{align*}
				\item Per definizione $\Lambda_{t,t+1}=\mathbb{E}\qty[\vb*{n}_t\transpose{\vb*{n}_{t+1}}]$, 
				quindi identicamente al punto precedente
				\begin{align*}
					\mathbb{E}\qty[\vb*{n}_t\transpose{\vb*{n}_{t+1}}]&=\Cov{\vb*{n}_t,\vb*{n}_{t+1}}+\mathbb{E}\qty[\vb*{n}_t]\cdot\mathbb{E}\qty[\transpose{\vb*{n}_{t+1}}]\\
					&=\Sigma_{t,t+1}+N^2\cdot\vb*{\mu}_t\transpose{\vb*{\mu}_{t,t+1}}\\
					&=N\cdot\qty(\vb*{\mu}_{t,t+1}+(N-1)\vb*{\mu}_t\transpose{\vb*{\mu}_{t+1}})
				\end{align*}
			\end{enumerate}%\qedsymbol
			ossia la tesi.\hfill\qedsymbol
		\end{block}

		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:prop_1}{\faHandPointLeft}}		
	\end{frame}

	\begin{frame}
		{\hypertarget{frame:dim_prop_noise_model:appendice}{Dimostrazione della proposizione~\ref{prop:noise_model}}}

		\begin{proof}
			\begin{enumerate}
				\item $\mathbb{E}\qty[\vb*{y}_t]=\mathbb{E}\qty[\,\mathbb{E}\qty[\vb*{y}_t\mid\vb*{n}_t]\,]=\mathbb{E}\qty[A_t\vb*{n}_t]=A_t\cdot\mathbb{E}\qty[\vb*{n}_t]$
				\item Si ha 
				\begin{align*}
					\mathbb{E}\qty[\vb*{y}_s\transpose{\vb*{y}_t}]&=\mathbb{E}\qty[\,\mathbb{E}\qty[\vb*{y}_s\transpose{\vb*{y}_t}\mid \vb*{n}_s,\vb*{n}_t]\,]
					=\mathbb{E}\qty[\,\mathbb{E}\qty[\vb*{y}_s\mid \vb*{n}_s,\vb*{n}_t]\cdot\mathbb{E}\qty[\transpose{\vb*{y}_t}\mid\vb*{n}_s,\vb*{n}_t]\,]\\
					&=\mathbb{E}\qty[\,\mathbb{E}\qty[\vb*{y}_s\mid \vb*{n}_s]\cdot\mathbb{E}\qty[\transpose{\vb*{y}_t}\mid\vb*{n}_t]\,]
					=\mathbb{E}\qty[\,A_s\cdot\vb*{n}_s\transpose{\vb*{n}_t}\cdot\transpose{A_t}\,]\\
					&=A_s\cdot\mathbb{E}\qty[\vb*{n}_s\transpose{\vb*{n}_t}]\cdot\transpose{A_t}
				\end{align*}
				\item Si ha, per i due punti precedenti,
				\begin{align*}
					\Cov{\vb*{y}_s,\vb*{y}_t}&=\mathbb{E}\qty[\vb*{y}_s\transpose{\vb*{y}_t}]-\mathbb{E}\qty[\vb*{y}_s]\cdot\mathbb{E}\qty[\transpose{\vb*{y}_t}]\\
					&=\mathbb{E}\qty[\vb*{y}_s\transpose{\vb*{y}_t}]-A_s\cdot\mathbb{E}\qty[\vb*{y}_t]\mathbb{E}\qty[\transpose{\vb*{y}_t}]\cdot\transpose{A_t}\\
					&=A_s\cdot\qty(\mathbb{E}\qty[\vb*{n}_s\transpose{\vb*{n}_t}]-\mathbb{E}\qty[\vb*{y}_t]\mathbb{E}\qty[\transpose{\vb*{y}_t}])\cdot\transpose{A_t}
				\end{align*}
			\end{enumerate}
			Dall'invertibilità di $A_s$ e $A_t$ segue la tesi.
		\end{proof}

		\blankfootnote{\textbf{Indietro:}~\hyperlink{frame:prop_noise_model}{\faHandPointLeft}}
	\end{frame}
 
    \begin{frame}
        {Dettagli sulla stocasticità di \texorpdfstring{$\hat{P}_{\textup{MoM}}$}{P_mom} 1/2}
        {\hypertarget{frame:P_mom_stochastic:appendice}{\texorpdfstring{$\hat{P}_{\textup{MoM}}$}{P_mom} è stocastica se e solo se \texorpdfstring{$\hat{\Sigma}\vb*{e}=\vb*{e}$}{Sigma*e=e}}}

        Si ha 
        \[
            \hat{P}_{\textup{MoM}}\vb*{e}=\vb*{e}\iff\qty(\frac{1}{N}\hat{\Sigma}+\hat{\vb*{\mu}}\transpose{\hat{\vb*{\mu}}})\vb*{e}=\operatorname{Diag}(\hat{\vb*{\mu}})\vb*{e}\iff\frac{1}{N}\hat{\Sigma}\vb*{e}+\langle\hat{\vb*{\mu}},\vb*{e}\rangle\cdot\hat{\vb*{\mu}}=\hat{\vb*{\mu}}
        \]
        \onslide<2->{Se $\hat{\vb*{m}}\ge 0$ allora $\norm{\hat{\vb*{m}}}_1=\langle\hat{\vb*{m}},\vb*{e}\rangle$, dunque se $\hat{\vb*{m}}\ge 0$ allora $\langle\hat{\vb*{\mu}},\vb*{e}\rangle=1$.}

        \onslide<3->{Dunque nel caso $\hat{\vb*{m}}\ge 0$ vale $\hat{P}_{\textup{MoM}}\vb*{e}=\vb*{e}\iff\hat{\Sigma}\vb*{e}=0$.}
        \medskip
        
        \onslide<4->{Il caso $\hat{\vb*{m}}<0$ potrebbe restituire $\hat{P}_{\textup{MoM}}$ con elementi negativi (è stato anche osservato sperimentalmente), ma non è chiaro se è possibile avere $\hat{P}_{\textup{MoM}}$ con elementi negativi anche quando $\hat{\vb*{m}}<0$.}

        \blankfootnote{\textbf{Indietro:}~\hyperlink{frame:P_mom_stochastic}{\faHandPointLeft}}
    \end{frame}

    \begin{frame}
        {Dettagli sulla stocasticità di \texorpdfstring{$\hat{P}_{\textup{MoM}}$}{P_mom} 2/2}
        {\hypertarget{frame:P_mom_stochastic_noise:appendice}{Considerazioni sui termini di rumore \texorpdfstring{$\langle\epsilon_t,\vb*{e}\rangle$}{e^t*eps_t}, \texorpdfstring{$\langle\vb*{m}_{\epsilon},\vb*{e}\rangle$}{e^t*m_{eps}^t}}}

        Per il caso $K=1$, rumore gaussiano, $\hat{\vb*{m}}\ge 0$, abbiamo ottenuto
        $\hat{\Sigma}=0\iff\langle\vb*{m}_{\epsilon},\vb*{e}\rangle\cdot\big((T+1)\hat{\vb*{m}}-\vb*{y}_T\big)+\langle\epsilon_1,\vb*{e}\rangle\cdot\hat{\vb*{m}}+\sum_{t=1}^{T-1}\langle\epsilon_{t+1},\vb*{e}\rangle\cdot\vb*{y}_t=0$
        \smallskip 
        
        Per ipotesi
        \begin{itemize}
            \item $\epsilon_t\independent\epsilon_s\quad\forall\,t\ne s\in[T]$, e $\epsilon_t(i)\sim\mathcal{N}(0,\sigma^2)\quad\forall\,t\in[T],\forall\,i\in[S]$
            \item $\epsilon_t=(\epsilon_t(i))_{i\in[S]}\in\R{S}$, con $\epsilon_t(i)\independent\epsilon_t(j)\quad\forall\,i\ne j$
        \end{itemize}
        Dunque $\langle\epsilon_t,\vb*{e}\rangle\sim\mathcal{N}(0,S\sigma^2)$ e di conseguenza 
        \[
            \P{\langle\epsilon_t,\vb*{e}\rangle\in[-\delta,\delta]}=\P{\frac{-\delta}{\sigma\sqrt{S}}\le Z\le\frac{\delta}{\sigma\sqrt{S}}}=2\Phi\qty(\frac{\delta}{\sigma\sqrt{S}})-1
        \]
        Similmente $\langle\vb*{m}_{\epsilon},\vb*{e}\rangle=\frac{1}{T}\sum_{t=1}^T\langle\epsilon_t,\vb*{e}\rangle\sim\mathcal{N}(0,\delta\frac{\sigma^2}{T^2})$ e 
        \[
            \P{\langle\vb*{m}_{\epsilon},\vb*{e}\rangle\in[-\delta,\delta]}=2\Phi\qty(\frac{\delta T}{\sigma\sqrt{S}})-1\xrightarrow{T\to\infty}1
        \]
        Il che dimostra che per $T\to\infty$ $\langle\vb*{m}_{\epsilon},\vb*{e}\rangle$ converge in probabilità a $\mathbb{E}\qty[\epsilon_t(i)]=0$.

        \blankfootnote{\textbf{Indietro:}~\hyperlink{frame:P_mom_stochastic}{\faHandPointLeft}}
    \end{frame}
    
	\begin{frame}
        {Consistenza di \texorpdfstring{$\hat{P}_{T,K}$}{P_T,K} per \texorpdfstring{$K\to\infty$}{K tendente a infinito}$\qquad 1/2$}
        {\hypertarget{frame:teorema1_lim_mhat:appendice}{Dettagli sul calcolo di \texorpdfstring{$\lim_K\hat{\vb*{m}}$}{un limite}}}

        %\begin{block}{}
        Nel calcolo di $\lim_K\hat{\vb*{m}}$ si fa uso dell'ipotesi di stazionarietà:
        \begin{align*}
            \lim_K\hat{\vb*{m}}&=\lim_K\frac{1}{T}\sum_{t=1}^T\hat{\vb*{m}}_t=\frac{1}{T}\sum_{t=1}^T\
            \lim_K\hat{\vb*{m}}_t=\frac{1}{T}\sum_{t=1}^T\mathbb{E}\qty[\vb*{y}_t]\
            =\frac{1}{T}\sum_{t=1}^T A_t\mathbb{E}\qty[\vb*{n}_t]\\
        \intertext{Per ipotesi $A_t=A\quad\forall\,t\in[T]$. Dunque}
            &=\frac{1}{T}\cdot A\cdot\sum_{t=1}^T\mathbb{E}\qty[\vb*{n}_t]=\frac{1}{T}\cdot A\cdot T\cdot\mathbb{E}\qty[\vb*{n}_1]=A\cdot\mathbb{E}\qty[\vb*{n}_1]=\mathbb{E}\qty[\vb*{y}_1]
        %\intertext{Ma per stazionarietà $\mathbb{E}\qty[\vb*{n}_t]=\mathbb{E}\qty[\vb*{n}_1]\quad\forall\,t\in[T]$. Dunque}
        \end{align*}
        poiché per stazionarietà $\mathbb{E}\qty[\vb*{n}_t]=\mathbb{E}\qty[\vb*{n}_1]\quad\forall\,t\in[T]$.
        %\end{block}

        \blankfootnote{\textbf{Indietro:}~\hyperlink{frame:teorema_1_part1}{\faHandPointLeft}}
    \end{frame}

    \begin{frame}
        {Consistenza di \texorpdfstring{$\hat{P}_{T,K}$}{P_T,K} per \texorpdfstring{$K\to\infty$}{K tendente a infinito}$\qquad 2/2$}
        {\hypertarget{frame:teorema1_lim_Sigmahat:appendice}{Dettagli sul calcolo di \texorpdfstring{$\lim_k\hat{\Sigma}_{t,t+1}$}{un altro limite}}}

        %\begin{block}{}
        Consideriamo 
        \[
            \hat{\Sigma}_{t,t+1}=\frac{1}{K}\sum_{k=1}^{K}\qty(\vb*{y}_t^{(k)}-\hat{\vb*{m}}_t)\cdot\transpose{{\qty(\vb*{y}_{t+1}^{(k)}-\hat{\vb*{m}}_{t+1})}}\kern-0.5em=\frac{1}{K}\sum_{k=1}^{K}\qty(\vb*{y}_t^{(k)}\cdot\transpose{{\vb*{y}_{t+1}^{(k)}}})-\hat{\vb*{m}}_t\cdot\transpose{\hat{\vb*{m}}_{t+1}}
        \]
        Per la legge dei grandi numeri $\hat{\vb*{m}}_t\xrightarrow{K\to\infty}\mathbb{E}\qty[\vb*{y}_t]$ q.c.,
        e $\frac{1}{K}\sum_{k=1}^K\vb*{y}_t^{(k)}\cdot\transpose{{\vb*{y}_{t+1}^{(k)}}}\xrightarrow{K\to\infty}\mathbb{E}\qty[\vb*{y}_t\cdot\transpose{\vb*{y}_{t+1}}]$ q.c.
        \medskip

        Dunque {\smaller (due applicazioni del teorema di Slusky~\cite[Theorem~5.5.17]{book:Berger_Casella})} $\hat{\vb*{m}}_t\cdot\transpose{\hat{\vb*{m}}}_{t+1}\xrightarrow{K\to\infty}\mathbb{E}\qty[\vb*{y}_t]\cdot\mathbb{E}\qty[\transpose{\vb*{y}}_{t+1}]$ in legge. Ma il valore atteso è una costante, quindi la convergenza è in probabilità. 

        Poiché la convergenza quasi certa implica quella in probabilità, si ha che 
        \[
            \hat{\Sigma}_{t,t+1}\xrightarrow{K\to\infty}\mathbb{E}\qty[\vb*{y}_t\cdot\transpose{\vb*{y}_{t+1}}]-\mathbb{E}\qty[\vb*{y}_t]\cdot\mathbb{E}\qty[\transpose{\vb*{y}}_{t+1}]=\Cov{\vb*{y}_1,\vb*{y}_2}
        \]
        in quanto per stazionarietà $\Cov{\vb*{y}_t,\vb*{y}_{t+1}}=\Cov{\vb*{y}_1,\vb*{y}_2}\quad\forall\,t\in[T]$.
        %\end{block}

        \blankfootnote{\textbf{Indietro:}~\hyperlink{frame:teorema_1_part1}{\faHandPointLeft}}
    \end{frame}

    \begin{frame}
        {Consistenza di \texorpdfstring{$\hat{P}_{T,K}$}{P_T,K} per \texorpdfstring{$T\to\infty$}{T tendente a infinito}$\qquad 1/3$}
        {\hypertarget{frame:teorema1_lim_gamma_n:appendice}{Dimostrazione di $\lim_{\tau}\gamma_{\vb*{n}}(\tau)=0$}}

        %\begin{block}{}
        Consideriamo $\qty(\gamma_{\vb*{n}}(\tau))_i=\Cov{\vb*{n}_t(i),\vb*{n}_{t+\tau}(i)}\equiv\gamma_{\vb*{n}(i)}(\tau)$.
        \begin{align*}
            \gamma_{\vb*{n}(i)}(\tau)&=\operatorname{Cov}\bigg(\sum_{m=1}^N\indicator,\sum_{m'=1}^N\indicator[t+\tau][m'][i]\bigg)\\
            %\begin{aligned}[t]
            &=\sum_{m=m'}\Cov{\indicator,\indicator[t+\tau][m][i]}+\sum_{m\ne m'}\cancel{\Cov{\indicator,\indicator[t+\tau][m'][i]}}\\
            %\end{aligned}\\
            &=N\Big(\mathbb{E}\qty[\indicator\indicator[t+\tau]]-\mathbb{E}\qty[\indicator]\mathbb{E}\qty[\indicator[t+\tau]]\Big)\\
            &=N\Big(\P{x_{t+\tau}^{m}=i,x_t^{m}=i}-\P{x_t^m=i}\P{x_{t+\tau}^m=i}\Big)\\
            &=N\Big(\P{x_{t+\tau}^m=i}{x_t^{m}=i}\P{x_t^m=i}-\vb*{\mu}_t(i)\cdot\vb*{\mu}_{t+\tau}(i)\Big)\\
            &=N\vb*{\mu}_t(i)\cdot\Big(P^{\tau}_{i,i}-\vb*{\mu}_{t+\tau}(i)\Big)\xrightarrow{\tau\to\infty}0
        \end{align*}
        in quanto per l'ergodicità $\lim_{\tau}\vb*{\mu}_{t+\tau}=\pi$ e $\lim_{\tau}P_{i,j}^{\tau}=\pi_j\quad\forall\,i,j\in[S]$.%\hfill$\square$
        %\end{block}


        \blankfootnote{\textbf{Indietro:}~\hyperlink{frame:teorema_1_part3}{\faHandPointLeft}}
    \end{frame}

    \begin{frame}
        {Consistenza di \texorpdfstring{$\hat{P}_{T,K}$}{P_T,K} per \texorpdfstring{$T\to\infty$}{T tendente a infinito}$\qquad 2/3$}
        {\hypertarget{frame:teorema1_yt_mean_ergodic:appendice}{Se \texorpdfstring{$\qty{\vb*{n}_t}_{t\in[T]}$}{n_t} è mean-ergodic, allora \texorpdfstring{$\qty{\vb*{y}_t}_{t\in[T]}$}{y_t} lo è}}

        \begin{itemize}
            \item Per la proposizione~\ref{prop:noise_model} si ha 
            \[
                \gamma_{\vb*{y}}(\tau)=\Cov{\vb*{y}_t,\vb*{y}_{t+\tau}}=A_t\cdot\qty(\Cov{\vb*{n}_t,\vb*{n}_{t+\tau}})\cdot\transpose{A}_{t+\tau}=A_t\cdot\gamma_{\vb*{n}}(\tau)\cdot\transpose{A}_{t+\tau}
            \]
            dunque $\lim_{\tau\to\infty}\gamma_{\vb*{n}}(\tau)=0\implies\lim_{\tau\to\infty}\gamma_{\vb*{y}}(\tau)=0$.
            \item Tramite la proposizione~\ref{prop:noise_model} si dimostra che 
            \[
                \Cov{\vb*{y}_s\cdot\transpose{\vb*{y}}_t,\vb*{y}_u\cdot\transpose{\vb*{y}}_v}=A_s\cdot\Cov{\vb*{n}_s\cdot\transpose{\qty(A_t\vb*{n}_t)},\qty(A_u\vb*{n}_u)\cdot\transpose{\vb*{n}}_v}\cdot\transpose{A}_v
            \]
            Inoltre per $\Cov{\vb*{n}_s\cdot\transpose{\qty(A_t\vb*{n}_t)},\qty(A_u\vb*{n}_u)\cdot\transpose{\vb*{n}}_v}_{i,j}$ si ha
            \begin{gather*}
                \operatorname{Cov}\Big(\vb*{n}_s(i)\cdot (A_t\vb*{n}_t)(j),\,(A_u\vb*{n}_u)(i)\cdot\vb*{n}_v(j)\Big)=\\
                \operatorname{Cov}\Big(\vb*{n}_s(i)\cdot\sum_{k}A_t(j,k)\vb*{n}_t(k),\,\vb*{n}_v(j)\cdot\sum_{k'}A_u(i,k')\vb*{n}_u(k')\Big)=\\
                \sum_{k,k'}A_t(j,k)\cdot A_u(i,k')\cdot\operatorname{Cov}\Big(\vb*{n}_s(i)\vb*{n}_t(k),\,\vb*{n}_u(k')\vb*{n}_v(j)\Big)
            \end{gather*}
            %\textbf{\textcolor{red}{Finire}}
        \end{itemize}

        \blankfootnote{\textbf{Indietro:}~\hyperlink{frame:teorema_1_part3}{\faHandPointLeft}}
    \end{frame}

    \begin{frame}
        {Consistenza di \texorpdfstring{$\hat{P}_{T,K}$}{P_T,K} per \texorpdfstring{$T\to\infty$}{T tendente a infinito}$\qquad 3/3$}
        {\hypertarget{frame:teorema1_yt_tp1_mean_ergodic:appendice}{Se \texorpdfstring{$\qty{\vb*{n}_t\cdot\transpose{\vb*{n}}_{t+1}}_{t\in[T]}$}{n_t n_{t+1}} è mean-ergodic, allora \texorpdfstring{$\qty{\vb*{y}_t\cdot\transpose{\vb*{y}}_{t+1}}_{t\in[T]}$}{y_t y_{t+1}} lo è}}
        
        Il caso che ci interessa è con gli istanti temporali $t\leftarrow t+1$, $s\leftarrow t$, $u\leftarrow t+\tau$, $v\leftarrow t+1+\tau$.
        \smallskip

        Modificando gli indici nella dimostrazione fornita nel materiale supplementare di~\cite{article:main}, si dimostra che 
        \[
            \lim_{\tau\to\infty}\operatorname{Cov}\Big(\vb*{n}_{t}(i)\vb*{n}_{t+1}(k),\vb*{n}_{t+\tau}(k')\vb*{n}_{t+1+\tau}(j)\Big)=0\qquad\forall\,k,k'\in[S].
        \]

        Dunque $\Cov{\vb*{n}_s\cdot\transpose{\qty(A_t\vb*{n}_t)},\qty(A_u\vb*{n}_u)\cdot\transpose{\vb*{n}}_v}_{i,j}\xrightarrow{\tau\to\infty}0\quad\forall\,i,j\in[S]$, 
        da cui segue che $\lim_{\tau\to\infty}\gamma_{\vb*{Z}}(\tau)=0$, ovvero $\qty{\vb*{y}_t\cdot\transpose{\vb*{y}}_{t+1}}_{t\in[T-1]}$ è mean-ergodic.


        \blankfootnote{\textbf{Indietro:}~\hyperlink{frame:teorema_1_part3}{\faHandPointLeft}}
    \end{frame}



\end{document}
